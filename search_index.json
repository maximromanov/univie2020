[
["index.html", "070172-1 UE Methodological course - Introduction to DH: Tools &amp; Techniques (2020W) Memex Edition Preliminaries", " 070172-1 UE Methodological course - Introduction to DH: Tools &amp; Techniques (2020W) Memex Edition Maxim G. Romanov 2021-01-06 Preliminaries This is a collection of relevant materials for a DH course by the Department of History, the University of Vienna. Course: 070172-1 UE Methodological course - Introduction to DH: Tools &amp; Techniques (2020W) Memex Edition u:find Link: https://ufind.univie.ac.at/en/course.html?lv=070172&amp;semester=2020W Meeting time: Tu 09:00-10:30 Meeting place: Seminarraum Geschichte 3 Hauptgebäude, 2.Stock, Stiege 9; due to COVID, all meetings will be held online Instructor: Dr. Maxim Romanov, maxim.romanov@univie.ac.at Language of instruction: English Office hours: Tu 14:00-15:00 (on Zoom; please, contact beforehand!) Office: Department of History, Maria-Theresien-Straße 9, 1090 Wien, Room 1.10 Back in 1945, Vannevar Bush, a Director of the US Office of Scientific Research and Development, proposed a device, which he called memex: Consider a future device … in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory. … The owner of the memex, let us say, is interested in the origin and properties of the bow and arrow. Specifically he is studying why the short Turkish bow was apparently superior to the English long bow in the skirmishes of the Crusades. He has dozens of possibly pertinent books and articles in his memex. First he runs through an encyclopedia, finds an interesting but sketchy article, leaves it projected. Next, in a history, he finds another pertinent item, and ties the two together. Thus he goes, building a trail of many items. Occasionally he inserts a comment of his own, either linking it into the main trail or joining it by a side trail to a particular item. When it becomes evident that the elastic properties of available materials had a great deal to do with the bow, he branches off on a side trail which takes him through textbooks on elasticity and tables of physical constants. He inserts a page of longhand analysis of his own. Thus he builds a trail of his interest through the maze of materials available to him. And his trails do not fade. Several years later, his talk with a friend turns to the queer ways in which a people resist innovations, even of vital interest. He has an example, in the fact that the outraged Europeans still failed to adopt the Turkish bow. In fact he has a trail on it. A touch brings up the code book. Tapping a few keys projects the head of the trail. A lever runs through it at will, stopping at interesting items, going off on side excursions. It is an interesting trail, pertinent to the discussion. … — The Atlantic, July 1945. "],
["memex-and-zettelkasten.html", "Memex and Zettelkasten On Memex On Zettelkasten", " Memex and Zettelkasten The memex machine is often thought of as a precursor of the Internet, where information is interconnected. Vannevar Bush, however, seems to have viewed more as a personal machine knowledge organization system, to use a modern term. A system that would facilitate coordination of relevant pieces of information into organized sequences that he himself called “trails”. The idea of a personal knowledge device is still of great relevance and of great importance to scholars and scientists whose job is to construct such trails on a daily basis. Needless to say that historians will particulary benefit from having such a machine at their disposal (as Bush’s example about the Turkish bow and the English longbow indicates, see (Bush 1945)). While it is not necessary to adhere slavishly to Bush’s vision, we can definitely use his vision as a springboad to develop something similar; something that will allow us to navigate the massive volumes of information, which grew significantly since the 1940s. In the following two sections you will find some relevant materials on memex and the history of this idea. Here, however, I want to take some time to think about how the design of our own memex should look like. What do we want from it? What can we reasonably achieve? Before I procede to that, I would like to dwell on another relevant and, in my opinion, closely connected idea — that of Zettelkasten. On its own, there is nothing particularly interestin and exciting about it, as the word refers to a rather unexciting piece of furniture: a “slip-box”, or a “card-box”. However, this term became closely associated with Niklas Luhmann, a German professor of sociology (U Bielefeld), who is considered one of the most prolific scholars of the 20th century. Like others, Luhmann himself attributed his productivity to his working method and the knowledge organization system which he implemented and systematically used throughout his career (See, Luhmann 1982; Luhmann and Baecker 1987). NB: Detailed bibliography can be found at the end of the section and in the References section (see, TOC). On Memex (Bush 1945) is the first article—“As We May Think”—that Vannevar Bush published on memex in The Atlantic. This animation imagines the way memex would have functioned (produced by the organizers of the Brown/MIT symposium). (Nyce and Kahn 1991) is the book that came out of the symposium held at the The Brown/MIT Vannevar Bush Symposium in 1995, celebrating the 50th anniversary of Bush’s groundbreaking article “As We May Think”. The book includes several Bush’s articles on memex that show the evolution of his thinking about this device. This book is difficult to find; someone made an EPUB version of it (also shared via Slack). Recordings of the symposium are available on YouTube:; other videos of this symposium can also be found at the Video Archive of The MIT/Brown Vannevar Bush Symposium https://www.dougengelbart.org/content/view/258/000/. I highly recommend you watch Paul Kahn: A Visual Tour of Vannevar Bush’s Work; other presentations are very interesting as well. (Park 2014) is an recent experiment (MA Thesis in Design), trying to create a version of memex. On Zettelkasten Niklas Luhmann on his Zettelkasten Niklas Luhmann was open about his working method, which he discussed in his interviews (Luhmann and Baecker 1987), and in some of his academic articles(Luhmann 1982). Others on Luhmann’s Zettelkasten https://niklas-luhmann-archiv.de/ … References "],
["work-plan.html", "Work Plan Memex-Building Cycle Research Cycle", " Work Plan A place for the detailed description of how we are going to construct our memex machine. Memex-Building Cycle Research Cycle Building Trails At first, all publication are converted into chain of minimal information units (MIU); ideally, these units should be paragraphs, in practice—pages). MIUs are sequentially connected with each other and can be read in their natural order. The graph below gives a visual representation of such organization: a, b, c, …, and h are publications, where numbers indicate MIU sequences. When we have such a structure, we can apply different analytical methods and connect MIUs that exhibit some measure of similarity. As a result, we may get a very different graph reresentation of connections. Some MIUs get connected into long chains; others—into clusters; yet others float completely disconnected. These are the new connections that we would want to explore in order to find new connections in the information that we study. These are Vannevar Bush’s trails. As we work our way through these trails, we discover that some are not particularly interesting, others are dead ends, but some are illuminating. We want to annotate and preserve them for later. Red lines indicate those vetted trails. These are Niklas Luhmann’s Folgezettel. "],
["lesson-01.html", "1 Lesson 01 1.1 Bibliography Managers 1.2 Zotero 1.3 Homework", " 1 Lesson 01 1.1 Bibliography Managers Bibliography managers make your life easier when it comes to collectin, organizing and maintaining bibliographical references and your library of electronic publications (most commonly as PDFs). Additionally, they are an indispensable writing tool as they take care of formatting (and reformatting) references and bibliographies in any writing project that you might undertake. There are plenty of different programs out there with their advantages and disadvantages (for example, Mendeley, RefWorks, Citavi, Endnote, Papers, Zotero, and quite a few more). We will use Zotero—it is being developed by scholars for scholars; it is free and open source; it does pretty much everything you might possibly need from a program of this kind. 1.2 Zotero 1.2.1 Getting Started Zotero can be installed from here: https://www.zotero.org/download/; the page will offer you a version suitable for your operating system, but you should also see the links to versions for specific systems (Mac OS, Windows, Linux). During installation Zotero should automatically integrate into your browser (like Chrome or Firefox) and into your word processor (MS Word, LibreOffice, GoogleDocs are supported). It is possible that you may have to do that manually. Zotero Connector for Chrome can be installed from the same page (https://www.zotero.org/download/) detailed explanations on how to use word processor plugins can be found here; you can use Zotero with MS Word, LibreOffice and Google Docs; in case you cannot get your plugin activated, check the Troubleshooting Section. 1.2.2 Main Functionality You need to be able to do the following tasks with your Zotero in order to take full advantage of its functionality. Online Tutorials: If you prefer video tutorials, you can check a series of tutorials prepared by the McGill Library (there are also plenty other tutorials on YouTube :); if you prefer to read, you can check a series of tutorials prepared by the UC Berkley Library. Adding bibliographical records (and PDFs) Using Zotero Connector: the easiest way to add a reference is from a browser with Zotero connector. This can be done practically from any library or journal database (e.g., Uni Wien Library, Worldcat.org, JSTOR); simply click the connector button while you are on a page with a publication that you want to add to your Zotero database. PDF may be automatically downloaded, if available; keep in mind that in places like JSTOR you need to agree to terms before this function will work; what you need to do is to download one PDF manually from a JSTOR page, where you will be asked to agree to terms of their services; Drag-and-dropping PDFs into Zotero; this however works only when Zotero can parse relevant bibliographical information from a PDF; This might be a good way to start if you already have lots of PDFs that you want to add to Zotero. Using Unique Identifiers: you can use ISBN or DOI numbers. Using Import: you can import bibliographical data from another application or from bibliographical files (formats, like RIS, which you can download from most libraries as well). Manually: you can manually add and fill in a record as well. Write-and-cite Detailed Instructions: MS Word, LibreOffice and Google Docs; you can also check the video tutorial. Add a citation Customize a citation (by adding prefixes, suffixes, page range for a specific reference, etc.). Change citation style. For example, change from Chicago Manual of Style to Universität Wien - Institut für Geschichte (Yes, there is this specific citation style for Zotero: https://www.zotero.org/styles?q=id%3Auniversitat-wien-institut-fur-geschichte); in order to do that you need to download the IfG style and install it into Zotero. You can find lots of different citation styles here: https://www.zotero.org/styles; to add a new style to Zotero: download the style you want. Open Zotero. Go to Preferences (under Zotero, Edit, or Tools — depending on your system). Click the “Cite” button. Click the “Styles” tab. Click the + button at the bottom right. Select the style file you saved in the first step. Generate and update bibliography in your paper. NB: If you use Zotero plugin for adding your citations, they remain connected to Zotero and can be automatically reformatted; you can also drag-and-drop any bibliographical record into any text editor—the reference will be formatted according to the currently selelected style, but it will not be connected to Zotero and cannot be reformatted automatically later. General Maintenance and Organization Zotero can [automatically] rename PDFs using metadata, although the default function is not very robust (see, Zotfile plugin below). You can create “collections” and drag-and-drop publications relevant to a specific topic or project you are working on. 1.2.3 Additional Functionality: Plug-Ins There is a variety of third-party plugins that you can add to Zotero for additional functionality. The list of plugins can be found at https://www.zotero.org/support/plugins. To install a plugin, you need to download its .xpi file to your computer. Then, in Zotero, click “Tools → Add-Ons”, then drag the .xpi for the plugin onto the Add-Ons window that opens. Two plugins will be of particular interest to us: Zotfile and BetterBibTeX. 1.2.4 Zotfile Zotfile (http://zotfile.com/) is a Zotero plugin to manage your attachments: automatically rename, move, and attach PDFs (or other files) to Zotero items, sync PDFs from your Zotero library to your (mobile) PDF reader (e.g. an iPad, Android tablet, etc.) and extract annotations from PDF files. This plugin is particularly helpful for organizing PDFs on your hard drive. By default, Zotero saves PDFs in a computationally safe, but humanely incomprehensible manner: each PDF, even if it is renamed from bibliographical metadata and is human readable, it is still placed into a folder whose name is a random sequence of characters. Zotfile allows you to organize PDFs in a more human-friendly manner. The first screenshot below shows Zotero default mode, while the second one shows Zotfile mode: essentially, Zotfile creates a folder for each author and PDFs of all publications by that author get placed in that folder. You can sync this folder with Dropbox or other cloud service and access it from your tablet or phone. Zotero default organization. Zotfile organization. 1.2.5 Better BibTeX for Zotero For a moment this will not be an immediately useful plug-in, but it is the most important one for our Memex project. This plugin exports bibliographical data into a bibTeX format, which is very easy to process with python scripts (it also generates citation keys which can be used for citation in markdown, which we will cover later). The two screenshots below show how the same record looks in Zotero preview and in the bibTeX format. A Record in Zotero. The Same Record in BibTeX Format. 1.3 Homework collect 30-50 bibliographic records into your Zotero (ideally with PDFs); the number may seem like a lot, but you will see that you can do that it will take only about 30 mins on JSTOR; those of you who are already using Zotero must already have more than 50 records in your databases. clearly, you should be collecting items that are relevant to your fields of study and your research; organize them into folders, if that is necessary; create Bibliography and email it to me (this is one-click operation; try to figure on your own how to do this; asking on Slack counts); make sure that you are comfortable with the main functionality of Zotero; that you have the discussed plugins installed; to get comfortable with the main functionality, you should practice each listed procedure at least a couple of times. in preparation for the next class, please, watch the following two short videos from Dr. Paul Vierthaler’s Hacking the Humanities series: Episode 1: Introduction to the Hacking the Humanities Tutorial Series and install Python via Anaconda; you can also install Python directly from https://www.python.org/, but Anaconda distribution might make your life easier, especially if you are on Windows. Episode 2: The Command Prompt. Submitting homework: Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. * In the subject of your email, please, add the following: CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber, where CCXXXXX is the numeric code of the course; LXX is the lesson for which the homework is being submitted; YourLastName is your last name, and YourMatriculationNumber is your matriculation number. "],
["lesson-02.html", "2 Lesson 02 2.1 Command Line 2.2 Homework", " 2 Lesson 02 2.1 Command Line The knowledge of “command line” opens a whole new world of opportunities, as the number of interface-less programs and applications is significantly larger; command line also offers a more robust and direct controls over a computer. The main goal is to learn the basics of this indispensable tool. We can use Terminal on Mac (installed), Powershell on MS Windows (should be installed), although other command line tools will work as well. Before we proceed, however, let’s discuss a few concepts: What a filesystem is How to run a program from the command line What it means to run a program How the computer knows what program to run How to refer to a file from the command line 2.1.1 The filesystem Every disk contains a filesystem and information about where disk data is stored and how it may be accessed by a user or application. A filesystem typically manages operations, such as storage management, file naming, directories/folders, metadata, access rules and privileges. Commonly used file systems include File Allocation Table 32 (FAT 32), New Technology File System (NTFS) and Hierarchical File System (HFS). All the files and programs on your computer are organized into folders; all these folders are in some other folders all the way down to your hard drive, which we call the root of your filesystem. Every hard drive, USB drive, DVD, and CD-ROM has its own filesystem. You normally look at the contents of your filesystem via the Finder (on Mac) or the Explorer (on Windows). Open a window there now. The Finder / Explorer window opens in some folder, which might be different depending on what computer operating system you’re using. But you’ll usually have a navigation bar to the left, that will let you go to different places. You see folders, also known as directories, and you might see files too. One thing that computer OSes like to hide from you is the fact that you have a home directory, where all your personal files and folders should live. This makes it easier for multiple users to use a single computer. You can find your home directory like this: On Mac, select Go &gt; Home in the menu. On Windows, click on Local Drive (C:), then click on Users, then click on your login name. You’ll see that your home directory has several folders in it already, that were created automatically for you when you first made a user account. Now how can you tell where you are, with respect to the root of your drive? On Mac, select View &gt; Show Path Bar in the menu. On Windows, look: The Finder / Explorer will also show you where in your computer’s filesystem you are. This is called the path—it shows you the path you have to take from the root of your filesystem to the folder you are in. Now if you are on Windows, click on that bar and you’ll see something surprising. This is your real path. The C:\\ is how Windows refers to the root of your filesystem. Also note that, even if your OS is not in English, the path may very well be! 2.1.2 Getting started with the command line Now that you have a hint of what is going on behind the scenes on your computer, let’s dive into the command line. Here is how you get there: On Mac, look for a program called Terminal.app On Windows, look for a program called Powershell By default, these shells open in your home directory. On Windows this is easy to see, but on Mac it is less clear—that is, until you know that this ~ thing is an alias for your home directory. 2.1.3 Components of the command line The command line consists of a prompt where you type your commands, the commands and arguments that you type, and the output that results from those commands. The prompt is the thing that looks like (where user is your username): MacBook-Pro:~ user$ or PS C:\\Users\\user&gt; You will never need to type the prompt. That means that, if you are noting down what we do in class for future reference, you should not copy this part! The prompt actually gives you a little bit of information. On Mac, it has the name of the computer, followed by a :, followed by the directory where you are, followed by your username, with $ at the end. On Windows, it has PS for PowerShell, followed by the name of the drive (C for most of you), followed by a :, followed by the full path to where you are, with &gt; at the end. When you type a command, nothing happens until you press the Return/Enter key. Some commands have output (more text that appears after you press Return/Enter) and others don’t. You cannot run another command until the prompt is given again. NOTE: From this point on, you will be running the commands that are run here! Let’s first make sure we are in our home directory by typing cd ~. For most of you this should change nothing, but now you know your first shell command. The cd stands for change directory, and what follows is the directory you want to go to. cd ~ Now let’s have a look around. The command to show what is in any particular directory is called ls, which stands for list. Try running it. ls If you are on Windows, what you get will look more like this: PS C:\\Users\\user&gt; ls You should then see something like: Verzeichnis: C:\\Users\\user Mode LastWriteTime Length Name ---- ------------- ------ ---- d---- 23.02.2016 21:18 .oracle_jre_usage d-r-- 23.02.2016 20:40 Contacts d-r-- 23.02.2016 20:40 Desktop d-r-- 23.02.2016 21:11 Documents d-r-- 23.02.2016 21:16 Downloads d---- 23.02.2016 21:24 exist d-r-- 23.02.2016 20:40 Favorites d-r-- 23.02.2016 20:40 Links d-r-- 23.02.2016 20:40 Music d-r-- 23.02.2016 20:40 Pictures d-r-- 23.02.2016 20:40 Saved Games d-r-- 23.02.2016 20:40 Searches d-r-- 23.02.2016 20:40 Videos PS C:\\Users\\user&gt; Now go into your documents folder and look around. cd Documents ls How does this compare to what you see in the Finder / Explorer window, if you click on the Documents folder? Another important command, which tells you where you are at any given time, is pwd. This means print working directory. Try it now and see what you get. pwd If ever you get lost on the command line, pwd will always help you find your way. 2.1.4 File paths and path notations By now you will have noticed that I’ve mentioned the path a few times, and that it seems to have something to do with this thing that pwd prints out. (And, most annoyingly, that it looks different on Mac and Windows) The bit of text that you get from pwd is what is called path notation, and it is very important that you learn it if you want to do anything with your own digital data. Here are some rules: The / (or \\\\ on Windows) separates folder names. So Desktop/Video means “the thing called Video inside the Desktop folder”. The / all by itself refers to the base of your hard drive (usually Macintosh HD or C:\\.) The ~ refers to your home folder. These things can be combined; ~/Documents means “the Documents folder in my home folder.” The . means “the current working directory”, i.e. what you would get if you ran the command pwd. The .. means “one directory back”—if pwd gives you /Users/user, then .. means /Users. If the path does not start with a . or a / or a ~, then it will be assumed to start with a ./, that is, “start from the current working directory.” Let’s wander around a bit. But, first, let’s download a zip file with somoe materials for this class. Unzip it somewhere and go to that folder in your Terminal or Powershell. cd /path/to/the/folder/tnt_practice_materials pwd cd ./cd 02_CommandLine/ pwd ls Try the following if you are on Mac ls -lh cd .. pwd NB: you can use TAB to autocomplete the path: type ls to see what folders are in Documents, then go to any one of them by typing cd (space) and then the first two letters &gt; after that use TAB and the name will be complete automatically. cd 03[TAB] pwd cd ../01[TAB] pwd ls McCarty_Modeling.pdf cd .. 2.1.5 Command line arguments So far we have learned three commands: cd, ls, and pwd. These are useful for navigation, but we can run a lot more commands once we learn them, and have a need for them! What are we doing, exactly? First word is the command All other words are the arguments Words must be separated by spaces cd is a command that expects an argument: the name of the directory you want to go to. But what if the name has a space in it? NB: You may think of most commands as sentences with subject, predicate, and object (or multiple objects). cd ./01_Zotero_Word/Green Eggs and Ham What happened there? Well, we have a folder called Green Eggs and Ham in our example, and we tried to go there. But since the command line works with arguments, and since arguments are separated by space, the machine interpreted this as if we were saying “Change to the ./01_Zotero_Word/Green folder, and then Eggs, and, Ham, whatever that means.” And it gave us an error, because we don’t have a folder called Green in our example. You can get around this. How you get around it depends on whether you’re on Windows or not. One way to get around it that should work both places is like this: On Windows: cd &#39;./01_Zotero_Word/Green Eggs and Ham&#39; On Mac (you need to escape spaces by adding a backslash in front of them): cd ./01_Zotero_Word/Green\\ Eggs\\ and\\ Ham/ NB: The easiest solution is to use TAB for autocomplete! 2.1.6 More commands With command line you can do everything that you became accustomed to be doing in a graphical interface of your favorite file manager. For example, you can copy, move, and delete files and folders. You can use: mv to move files rm (on Windows also: del) to remove/delete files cp to copy files In all cases you need to state which files you want to mv, rm, or cp. In some cases you also need to point where you want to mv or cp your files. NB: Syntax on Mac and Windows will vary slightly, but if you keep using [TAB] for autocompletion, there will be no different in the process of typing the command, so let’s try to do it this way. To start, let’s go to the root directory of our course materials. From there, let’s do the following: cd 01[TAB] ls cp Mc[TAB] Green[TAB] cd Green[TAB] ls NB: when you hit [TAB] after Mc you are not going to get the full autocomplete, because there are two files that start with McCarty_Modeling—one is pdf and another—txt. You will need to type one more letter p and then hit [TAB] again to get the file name that you need. Thus, the command can be transcribed as: M[TAB]p[TAB] Now let’s rm (delete) the McCarty_Modeling.pdf from this folder, then go to the folder where we copied it, and then mv (move) it back to where it was in the first place. rm M[TAB]p[TAB] ls cd G[TAB] mv Mc[TAB] ../ cd .. ls Tada! The McCarty_Modeling.pdf should now be back where it was. If you want to learn about new commands, try to google. Googling things like this is a very big part of being a DH scholar! You will most likely find your answers on https://stackoverflow.com/, which will become your most frequented resource, if you embark on the DH path. 2.2 Homework Command line Watch again a short video on Command Prompt Dr. Vierthaler’s Hacking the Humanities series: Episode 2: The Command Prompt. Work through the following materials on command line which is relevant to your operating system. Ted Dawson, “Introduction to the Windows Command Line with PowerShell,” The Programming Historian 5 (2016), https://programminghistorian.org/en/lessons/intro-to-powershell. Ian Milligan and James Baker, “Introduction to the Bash Command Line,” The Programming Historian 3 (2014), https://programminghistorian.org/en/lessons/intro-to-bash. Python Work through Chapter I of Zelle’s book; read the entire chapter; retype and run all code snippets as described in the book; work through the chapter summary and exercises; complete all programming exercises; For submission: email me the results of “Programming Exercises”. In your submission there should be text files or python script files for exercises 1 (results of print function), 3, 4, 5, 7. Each python script should be working, i.e. you should be able to run it and get relevant results. You are welcome to discuss any of these assignments on Slack. Work through the following videos from Dr. Vierthaler’s Hacking the Humanities series: Episode 3: The Very Basics of Python Episode 4: Strings Episode 5: Integers, Floats, and Math in Python NB: The best way to work through these tutorials is to repeat all steps after the instructor. You can find the scripts at https://github.com/vierth/humanitiesTutorial. Submitting homework: Homework assignment must be submitted by the beginning of the next class; Email your homework to the instructor as attachments. In the subject of your email, please, add the following: CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber, where CCXXXXX is the numeric code of the course; LXX is the lesson for which the homework is being submitted; YourLastName is your last name, and YourMatriculationNumber is your matriculation number. "],
["lesson-03.html", "3 Lesson 03 3.1 Version Control and Collaboration 3.2 Setting-up git 3.3 General git workflow 3.4 Main git Commands 3.5 Some useful command line commands to remember 3.6 Practice 3.7 Homework", " 3 Lesson 03 3.1 Version Control and Collaboration Version control systems are extremely helpful for the development of DH projects, which are often lengthy and complex and require organic collaboration. Git and GitHub are currently the most popular tools of this kind. It is difficult to imagine a DH project that would not rely on the use of git and GitHub. Before we begin, make sure to: Create a github account at https://github.com/, if you do not have one yet. Download and install git software: for Windows: you can download it from https://git-scm.com/download/win. Please, choose 64-bit Git for Windows Setup. you can also install a portable version of git which does not require installation https://git-scm.com/download/win. For this, choose 64-bit Git for Windows Portable. Simply download and unzip (Suggestion: move that unzipped folder to the folder where you keep all class-related files and materials). In the folder, run git-bash.exe (for a more Unix-like command line) or git-cmd.exe (for Windows command line). for Mac: try to run git --version from Terminal. If git is not installed, you will be prompted to install Xcode Command Line Tools which comes with git among other things. This is the easiest way. Note: there are also interface tools for github. We will not be working with them in the class, but you are welcome to test them on your own at home. See, https://desktop.github.com. The main reason for this is because interface tools will be different for different operating systems, while the command line usage will be exactly the same across all platforms. In class we will cover the following: Basic git functionality; Starting a github-based website; Basics of markdown; 3.2 Setting-up git git config --global user.name \"YourName\" git config --global user.email \"YourEmail\" 3.3 General git workflow In Terminal (on Mac) or Git-Bash (on Windows) create a repository under your account online at https://github.com. Alternatively, you can also fork somebody else’s repository.1 clone (NB: this is done on https://github.com!) work add commit push / pull send pull request (NB: this is done on https://github.com) Note: Steps 2 and 8 are relevant only when you work on a project (repository) that is owned by somebody else. If you work on a repository that you created under your account, you only need steps 1, 3-7. Below is a visual representation of this cycle. 3.4 Main git Commands git clone &lt;link&gt; clones/downloads a repository on you machine git status shows the current status of the repository (new, changed, deleted) git add . adds all new files and modified files to the repository git commit -m \"message\" saves all files in their current state into the repository, and created a milestone git push origin master uploads changes to https://github.com origin is a specific repository you are pushing your changes to; it is automatically set up, when you clone a repository on your computer. master is the branch you are pushing to the repository; master is the default name of the main branch in a git repository. To check the names of your branches, you can type git branch. NB: sometimes you may get an error, which in most cases means that you need to pull first git pull origin master downloads changes from https://github.com git log shows the history of commits; here you can choose where you want to roll back, in case of troubles. 3.5 Some useful command line commands to remember pwd shows you where you are on a drive (gives you path) ls / dir [on Windows] shows everything in the your current location/folder cd &lt;name of the folder&gt; takes you to that folder cd .. takes you one level up in the tree structure of your computer 3.6 Practice Under your GitHub account, create repository HW070172; clone it to your computer (use command line: git clone LinkToYourRepository); Now, in the repository: let’s edit README.md (create it, if you have not yet); add some text into this file create subfolders for Lessons, like L01, L02, L03, etc. copy/paste your homework files in respective subfolders. Now, do the add-commit-push routine to upload the files to your repository Now, online: check if your files are there let’s do some edits to the README.md file (markdown basics / github flavor) pull / push 3.7 Homework Git and GitHub Watch a video on Git &amp; GitHub in Dr. Vierthaler’s Hacking the Humanities series: Supplement 1: A quick Git and Github Tutorial. This will help you to go over the new material and pick up a few more useful git &amp; gitHub tricks. There is an interface for github that you can also use, but I strongly recommend to use command line; interfaces change, but commandline commands remain the same! Daniel van Strien. 2016. “An Introduction to Version Control Using GitHub Desktop,” The Programming Historian 5, https://programminghistorian.org/. Please, also read (for markdown): Simpkin, Sarah. 2015. “Getting Started with Markdown.” Programming Historian, November. https://programminghistorian.org/. More on github-flavored markdown: https://guides.github.com/features/mastering-markdown/. On markdown for academic writing, see https://pandoc.org/MANUAL.html. A cheat-sheet &amp; interactive tutorial for your practice: https://commonmark.org/help/. Extra: you can build and host a website on github.com; your website will have the name: YourUserName.github.io — you can create a repository with that name and build your website there using Jekyll and GitHub Pages. Any other repository may also be converted into a part of your website, which will be accessible at YourUserName.github.io/YourRepository/ Visconti, Amanda. 2016. “Building a Static Website with Jekyll and GitHub Pages.” Programming Historian, April. https://programminghistorian.org/. Python Work through Chapter II of Zelle’s book; read the entire chapter; retype and run all code snippets as described in the book; work through the chapter summary and exercises; complete all programming exercises; Watch Dr. Vierthaler’s videos: Episode 04: Strings; Episode 05: Integers, Floats, and Math in Python; Episode 06: Lists Submitting homework Homework assignment must be submitted by the beginning of the next class; Now, that you know how to use GitHub, you will be submitting your homework pushing it to github: Create a relevant subfoler in your HW070172 repository and place your HW files there; push them to your GitHub account; Email me the link to your repository with a short message (Something like: I have completed homework for Lesson 3, which is uploaded to my repository … in subfolder L03) In the subject of your email, please, add the following: CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber, where CCXXXXX is the numeric code of the course; LXX is the lesson for which the homework is being submitted; YourLastName is your last name, and YourMatriculationNumber is your matriculation number. NB: this is done on https://github.com); forking means creating your own copy of some one’s repository at that specific moment in time↩ "],
["lesson-04.html", "4 Lesson 04 4.1 Sustainable Academic Writing with markdown, pandoc, and the *TeX family 4.2 Class Notes 4.3 pandoc Commands 4.4 Writing with markdown in Atom 4.5 Reference Materials: 4.6 Homework", " 4 Lesson 04 4.1 Sustainable Academic Writing with markdown, pandoc, and the *TeX family Introduction to sustainable academic writing that avoids any proprietary software solutions and formats. Before the class, make sure to install the following. pandoc (follow instructions on https://pandoc.org/installing.html) LaTeX engine (install from here: https://miktex.org/) LaTeX for Mac: MikTeX seems to be very finicky with Macs. The following solution proves to be more manageable and stable (you need to run this command in Terminal): brew install librsvg python homebrew/cask/basictex (from Pandoc page); after that missing packages might have to be installed manually, but that is relatively easy — the sustem will prompt you to install them, and that needs to be done only once; alternatively, one can install MacTeX (this one is quite large, about 4Gb). markdown 4.2 Class Notes Files: Download the following archive file: sustainable_writing.zip. Make sure to unzip it! It contains the following files: biblio.bib—a bibliography file; cms-fullnote.csl—a citation style; main.md—the main text file (its contents are also shown below); NB: remember that all files must be in the same folder; it makes sense to put folders into a subfolder (not to overcrowd your main folder), but then do not forget to change the path in your image code. TEXT for your main.md file. --- title: | *From*: &quot;Modeling: A Study in Words and Meanings&quot; by Willard McCarty subtitle: author: date: \\today bibliography: biblio.bib csl: cms-fullnote.csl --- &gt;&gt; Out on site, you were never parted from your plans. They were your Bible. They got dog-eared, yellowed, smeared with mud, peppered with little holes from where you had unrolled them on the ground. But although so sacred, the plans were only the start. Once you got out there on the site everything was different. No matter how carefully done, the plans could not foresee the *variables*. It was always interesting, this moment when you saw for the first time the actual site rather than the idealised drawings of it. &gt;&gt; Kate Grenville, *The Idea of Perfection* (Sydney: Picador, 1999): 62–3 # Introduction The question of modeling arises naturally for humanities computing from the prior question of what its practitioners across the disciplines have in common. What are they all doing with their computers that we might find in their diverse activities indications of a coherent or cohesible practice? How do we make the best, most productive sense of what we observe? There are, of course, many answers: practice varies from person to person, from project to project, and ways of construing it perhaps vary even more. In this chapter I argue for modeling as a model of such a practice. I have three confluent goals: to identify humanities computing with an intellectual ground shared by the older disciplines, so that we may say how and to what extent our field is of as well as *in* the humanities, how it draws from and adds to them; at the same time to reflect experience with computers &quot;in the wild&quot;; and to aim at the most challenging problems, and so the most intellectually rewarding future now imaginable. My primary concern here is, as Confucius almost said, that we use *the correct word* for the activity we share lest our practice go awry for want of understanding (*Analects 13.3*). Several words are on offer. By what might be called a moral philology I examine them, arguing for the most popular of these, &quot;modeling.&quot; The nominal form, &quot;model&quot;, is of course very useful and even more popular, but for reasons I will adduce, its primary virtue is that properly defined it defaults to the present participle, its semantic lemma. Before getting to the philology I discuss modeling in the light of the available literature and then consider the strong and learned complaints about the term. # Background Let me begin with provisional definitions[^1]. By &quot;modeling&quot; I mean *the heuristic process of constructing and manipulating models*, a &quot;model&quot; I take to be either *a representation of something for purposes of study*, or *a design for realizing something new*. These two senses follow Clifford Geertz&#39;s analytic distinction between a denotative &quot;model *of*&quot; such as a grammar describing the features of a language, and an exemplary &quot;model *for*&quot; such as an architectural plan [@geertz_interpretation_2017, 93][^2]. In both cases, as the literature consistently emphasizes, a model is by nature a simplified and therefore fictional or idealized representation, often taking quite a rough-and-ready form: hence the term &quot;tinker toy&quot; model from physics, accurately suggesting play, relative crudity, and heuristic purpose [@cartwright_how_1984, 158]. By nature modeling defines a ternary relationship in which it mediates epistemologically, between modeler and modeled, researcher and data or theory and the world [@morgan_models_1999]. Since modeling is fundamentally relational, the same object may in different contexts play either role: thus, e.g., the grammar may function prescriptively, as a model for correct usage, the architectural plan descriptively, as a model of an existing style. The distinction also reaches its vanishing point in the convergent purposes of modeling: the model of exists to tell us that we do not know, the model for to give us what we do not yet have. Models *realize*. [^1]: My definitions reflect the great majority of the literature explicitly on modeling in the history and philosophy of the natural sciences, especially of physics. The literature tends to be concerned with the role of modeling more in formal scientific theory than in experiment. The close relationship between modeling and experimenting means that the rise of a robust philosophy of experiment since the 1980s is directly relevant to our topic; see [@hacking_stability_1988]. Quite helpful in rethinking the basic issues for the humanities are the writings from the disciplines other than physics, e.g., [@clarke_models_2015] on archaeology; on the social sciences, the essays by de Callatay, Mironesco, Burch, and Gardin in [@franck_explanatory_2011]. For interdisciplinary studies see Shanin (1972) and [@morgan_models_1999], esp. &quot;Models as Mediating Instruments&quot; (pp. 10–37). [^2]: Cf. Goodman&#39;s distinction between &quot;denotative&quot; and &quot;exemplary&quot; models, respectively (1976: 172–3); H. J. Groenewold&#39;s &quot;more or less poor substitute&quot; and &quot;more or less exemplary ideal&quot; (1960: 98). Similar distinctions are quite common in the literature. # Bibliography 4.3 pandoc Commands NB: On Windows, you may see a pop-up Windows from MikTex asking to download a missing package for LaTeX. This means that some package is missing and you need to download it (or several of them). Uncheck a birdie to install all necessary packages at once. After that everything should work. First try to convert to docx or html. These two formats do not require pandoc -f markdown -t docx -o main.docx --filter pandoc-citeproc main.md pandoc -f markdown -t html -o main.html --filter pandoc-citeproc main.md pandoc -f markdown -t epub -o main.epub --filter pandoc-citeproc main.md pandoc -f markdown -t latex -o main.pdf --filter pandoc-citeproc main.md NB: it may so happen that your version of pandoc will complain about --filter pandoc-citeproc. If that happens, your commands should look like the following: pandoc -f markdown -t docx -o main.docx --citeproc main.md pandoc -f markdown -t html -o main.html --citeproc main.md pandoc -f markdown -t epub -o main.epub --citeproc main.md pandoc -f markdown -t latex -o main.pdf --citeproc main.md Comment: -f means “convert from a specific format”. -t means “convert to a specific format”. Thus, the whole command (say, the first one) reads as follows: pandoc converts from (-f) markdown to (-t) latex, then outputs (-o) main.pdf, to which a ‘filter’ that processes citations (--filter pandoc-citeproc) is applied; and the file to which this all is applied is main.md. 4.4 Writing with markdown in Atom Atom (https://atom.io/) allows one to integrate writing in markdown with the helpfulness of Zotero, as well as offers quite a few other nice features. You can find how to set everything up in Scott Selisker’s blog post: http://u.arizona.edu/~selisker/post/workflow/. 4.5 Reference Materials: Simpkin, Sarah. 2015. “Getting Started with Markdown.” Programming Historian, November. https://programminghistorian.org/lessons/getting-started-with-markdown. Tenen, Dennis, and Grant Wythoff. 2014. “Sustainable Authorship in Plain Text Using Pandoc and Markdown.” Programming Historian, March. https://programminghistorian.org/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown. 4.6 Homework Convert a plain text paper into markdown and convert it with Pandoc into a PDF, MS Word, and HTML documents. Plain text file for the task: McCarty_Modeling.txt Use this PDF file as a guide for your formatting: McCarty_Modeling.pdf Convert only the first 7 pages. You can skip up to 1/3 of bibliographical records, if you cannot find them online. Alternatively, you can use any of your own papers that you have already written: 5 pages, 10 footnotes, 5 bibliography items. Python Work through Chapters 3 and 5 of Zelle’s book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 3; 1-7 in Chapter 5; Watch Dr. Vierthaler’s videos: Episode 07: Booleans (and Boolean Operators) Episode 08: Loops (and file objects) Submitting homework: Homework assignment must be submitted by the beginning of the next class; Now, that you know how to use GitHub, you will be submitting your homework pushing it to github: Create a relevant subfoler in your HW070172 repository and place your HW files there; push them to your GitHub account; Email me the link to your repository with a short message (Something like: I have completed homework for Lesson 3, which is uploaded to my repository … in subfolder L03) In the subject of your email, please, add the following: CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber, where CCXXXXX is the numeric code of the course; LXX is the lesson for which the homework is being submitted; YourLastName is your last name, and YourMatriculationNumber is your matriculation number. "],
["lesson-05.html", "5 Lesson 05 5.1 Constructing Robust Searches 5.2 regular expressions 5.3 Class materials 5.4 Digital materials 5.5 Reference Materials 5.6 Homework", " 5 Lesson 05 5.1 Constructing Robust Searches 5.2 regular expressions In this lesson we will learn about regular expressions, an important semi-language for constructing complex searches. Any text editor that supports regular expressions will work fine for this lesson, but let’s all use Sublime Text (both Mac and Windows). Let’s use the following practicum files (Right Click &gt; Save File as ...) for the in-class practice: 1) version for training; 2) version with answers. Open the practicum file in Sublime Text. 5.2.1 What are regular expressions?** very small language for describing textual patterns not a programming language, yet a part of each one incredibly powerful tool for find/replace operations old (1950s-60s) “arcane art” ubiquitous Source: https://xkcd.com/208/ 5.2.2 What would we use regular Expressions for? to search: all spelling variations of the same word: Österreich, Osterreich or Oesterreich.. words of specific morphological patterns: [search], [search]er, [search]ed, [search]ing, [search]es: all derivatives from the same root/word entities that may be referred to differently: references to Vienna in different languages? (Wien, Vienna, Вена, فيينا, etc.) references to Austria? (Vienna, Graz, Linz, Salzburg, Innsbruck, etc.) references to concepts: references to education in biographies: “s/he graduated from”, “s/he studied”, etc. to search and replace: reformat “dirty”/inconsistent data (OCR output, for example) to tag: make texts navigable and more readable tag information relevant to your research and many other uses… 5.2.3 The Basics A regular expression (can be shortened as regex or regexp) is a sequence of symbols and characters expressing a string or pattern to be searched for within a longer piece of text. In this sequence there are characters that match themselves (most characters) and there are characters that activate special functionality (special characters). For example: Vienna is a regular expression that matches “Vienna”; “Vienna” is a pattern; Question: if the pattern at matches strings with “a” followed by “t”, which of the following strings will it match?2 at hat that atlas aft Athens 5.2.4 Characters &amp; Special Characters most characters match themselves. matching is case sensitive. special characters: ()^${}[]\\|.+?*. to match a special character in your text, you need to “escape it”, i.e. precede it with “” in your pattern: – Osterreich [sic] **does not* match “Osterreich [sic]”. – Osterreich \\[sic\\] matches “Osterreich [sic]”. 5.2.5 Character Classes: [] characters within [] are choices for a single-character match; think of this as a type of either or. the order within [] is unimportant. x[01] matches “x0” and “x1”. [10][23] matches “02”, “03”, “12” and “13”. initial ^ negates the class: – [^45] matches any character except 4 or 5. Question: if the pattern [ch]at matches strings with “c” or “h” followed by “a”, and then by “t”, which of the following strings will this regular expression match?3: that at chat cat fat phat 5.2.6 Ranges (within classes) Ranges define sets of characters within a class. – [1-9] matches any number in the range from 1 to 9 (i.e., any non-zero digit) – [a-zA-Z] matches any letter of the English alphabet (ranges for specific languages will vary) – [12][0-9] matches numbers between 10 and 29 (i.e., the first digit is either 1 or 2; the second one—any digit) Ranges shortcuts Shortcut Name Equivalent Class \\d digit [0-9] \\D not digit [^0-9] \\w word [a-zA-Z0-9_] (actually more) \\W not word [^a-zA-Z0-9_] (actually more) \\s space [\\t\\n\\r\\f\\v ] \\S not space [^\\t\\n\\r\\f\\v ] . everything [^\\n] (depends on mode) Question: if the pattern /\\d\\d\\d[- ]\\d\\d\\d\\d/ matches strings with a group of three digits, followed by a space or a dash, and then—by another group of four digits, which of the following strings will this regular expression match?4: 501-1234 234 1252 652.2648 713-342-7452 PE6-5000 653-6464x256 5.2.7 Repeaters these special characters indicate that the preceding element of the pattern can be repeated in a particular manner: runs? matches “runs” or “run” 1\\d* matches any number beginning with “1” repeater count ? zero or one + one or more * zero or more {n} exactly n times {n,m} between n and m times {,m} no more than m times {n,} no less than n times Question: We have several patterns, which strings will they match?5 Patterns A) ar?t B) ar*t C) a[fr]?t D) ar+t E) a.*t F) a.+t Strings 1) “at” 2) “art” 3) “arrrrt” 4) “aft” 5.2.8 Lab: Intro (in the practicum file). 5.2.9 Anchoring anchors match between characters. anchors are used to assert that the characters you’re matching must appear in a certain place. for example, \\bat\\b matches “at work” but not “batch”. Anchor matches… ^ the beginning of a line or a string $ the end of a line of a string \\b word boundary \\B not word boundary 5.2.10 Alternation: “|” (pipe) in regex, “|” means “or” on the US keyboard layout, this character is in the vicinity of “Enter” and “Right Shift”. you can put a full expression to the left of the pipe and another full expression to the right, so that either one could match: seek|seeks|sought matches “seek”, “seeks”, or “sought”. seeks?|sought matches “seek”, “seeks”, or “sought”. 5.2.11 Grouping everything within ( … ) is grouped into a single element for the purposes of repetition or alternation: the expression (la)+ matches “la”, “lala”, “lalalala” (but not “all”). schema(ta)? matches “schema” and “schemata” but not “schematic”. grouping example: what regular expression would match “eat”, “eats”, “ate” and “eaten”? eat(s|en)?|ate NB: we can make it more precise by adding word boundary anchors to exclude what we do not need, like, for example, words “sate” and “eating”: \\b(eat(s|en)?|ate)\\b. 5.2.12 Lab: Part I (in the practicum file). 5.2.13 Replacement regular expressions are most often used for search/replace operations in text editors: Search Window: search pattern Replace Window: replacement pattern 5.2.14 Capture during searches, ( … ) groups capture patterns for use in replacement. special variables \\1, \\2, \\3, etc. contain the capture (in some text editors: $1, $2, $3). if we apply (\\d\\d\\d)-(\\d\\d\\d\\d) to “123-4567”: – \\1 (or, $1) captures “123” – \\2 (or, $2) captures “4567” 5.2.15 Capture &amp; Reformat How to convert “Schwarzenegger, Arnold” to “Arnold Schwarzenegger”? Search: (\\w+), (\\w+) Replace (a): \\2 \\1 Replace (b): $2 $1 NB: (!) Before hitting “Replace”, make sure that your match does not catch what you do NOT want to change 5.2.16 Lab: Part II (in the practicum file). Finding toponyms (placenames): very simple: Construct regular expressions that find references to all Austrian cities.6 a bit tricky: Construct regular expression that finds only cities from 1) Lower Austria; 2) Salzburg.7 5.2.17 To keep in mind regular expressions are “greedy,” i.e. they tend to catch more than you may need. Always test! test before applying! (In text editors Ctrl+Z (Win), Cmd+Z (Mac) can help to revert changes) check the language/application-specific documentation: some common shortcuts are not universal (for example, some languages/applications use \\1 to refer to groups, while others use $1 for the same purpose). 5.3 Class materials Presentation with all the slides: PDF (Windows PowerPoint Format) 5.4 Digital materials Online references: http://www.regular-expressions.info/ http://ruby.bastardsbook.com/chapters/regexes/ Interactive tutorial: http://regexone.com/ Cheat Sheets: http://krijnhoetmer.nl/stuff/regex/cheat-sheet/ http://www.rexegg.com/regex-quickstart.html 5.5 Reference Materials Goyvaerts, J. and Levithan, S. (2012). Regular Expressions Cookbook. Second edition. Beijing: O’Reilly. Amazon Link. Friedl, J. E. F. (2006). Mastering Regular Expressions. 3rd ed. Sebastapol, CA: O’Reilly. Amazon Link (I will share PDFs of these books via Slack; I strongly recommend to flip through the first book just to get an idea of what kind of things one can do with regular expressions.) 5.6 Homework Finish the practicum; push your answers to your github repository. Python Work through Chapters 6 and 7 of Zelle’s book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 6; 1-9 in Chapter 7; Watch Dr. Vierthaler’s videos: Episode 09: Dictionaries Episode 10: Putting it Together (Analyses) Episode 11: Errors (reading and handling) Note: the sequences are somewhat different in Zelle’s textbook and Vierthaler’s videos. I would recommend you to always check Vierthaler’s videos and also check videos which cover topics that you read about in Zelle’s book. Submitting homework: Homework assignment must be submitted by the beginning of the next class; Now, that you know how to use GitHub, you will be submitting your homework pushing it to github: Create a relevant subfoler in your HW070172 repository and place your HW files there; push them to your GitHub account; Email me the link to your repository with a short message (Something like: I have completed homework for Lesson 3, which is uploaded to my repository … in subfolder L03) In the subject of your email, please, add the following: CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber, where CCXXXXX is the numeric code of the course; LXX is the lesson for which the homework is being submitted; YourLastName is your last name, and YourMatriculationNumber is your matriculation number. Matches are highlighted: at, hat, that, atlas, aft, Athens.↩ Matches are highlighted: that, at, chat, cat, fat, phat.↩ Matches are highlighted: 501-1234, 234 1252, 652.2648, 713-342-7452, PE6-5000, 653-6464x256.↩ ar?t matches “at” and “art” but not “arrrt”; a[fr]?t matches “at”, “art”, and “aft”; ar*t matches “at”, “art”, and “arrrrt”; ar+t matches “art” and “arrrt” but not “at”; a.*t matches anything with an ‘a’ eventually followed by a ‘t’.↩ Solution: Simply connect all toponyms from the list with a pipe symbol “|”↩ Solution 1: \\b([\\w ]+) \\(Lower Austria\\) — for Lower Austria; \\b([\\w ]+) \\(Salzburg\\) — for Salzburg; Solution 2 (cooler): \\b([\\w ]+)(?=( \\(Lower Austria\\))) — for Lower Austria; \\b([\\w ]+)(?=( \\(Salzburg\\))) — for Salzburg.↩ "],
["lesson-06.html", "6 Lesson 06 6.1 Understanding Structured Data 6.2 Larger Examples 6.3 In-Class Practice 6.4 Homework 6.5 Homework Solution", " 6 Lesson 06 6.1 Understanding Structured Data Doing Digital Humanities practically always means working with structured data of some kind. In most general terms, structured data means some explicit annotation or classification that the machine can understand, and therefore — effectively use. When we see the word “Vienna”, we are likely to automatically assume that this is the name of the capital of Austria. The machine cannot know that, unless there is something else in the data that allows it to figure it out (here, an XML tag): &lt;settlement country=\"Austria\" type=\"capital city\"&gt;Vienna&lt;/settlement&gt; — from this annotation (and its attributes) the machine can be instructed to interpret the string Vienna as a settlement of the type capital city in the country of Austria. It is important to understand most common data formats in order to be able to create and generate them as well as to convert between different formats. When we decide which format we want to work with, we need to consider the following: the ease of working with a given format (manual editing); suitability for specific analytical software; human-friendliness and readability; open vs. proprietary. In general, it does not make any sense to engage in the format wars (i.e., one format is better than another); one should rather develop an understanding that almost every format has its use and value in specific contexts or for specific tasks. What we also want is not to stick to a specific format and try to do everything with it and only it, but rather to be able to write scripts with which we can generate data in suitable formats or convert our data from one format into another. Let’s take a look at a simple example in some most common formats. 6.1.1 XML (Extensible Markup Language) &lt;note&gt; &lt;to&gt;Tove&lt;/to&gt; &lt;from&gt;Jani&lt;/from&gt; &lt;heading&gt;Reminder&lt;/heading&gt; &lt;body&gt;Don’t forget me this weekend!&lt;/body&gt; &lt;/note&gt; 6.1.2 CSV/TSV (Comma-Separated Values/ Tab-Separated Values) to,from,heading,body Tove,Jani,Reminder,Don’t forget me this weekend! to,from,heading,body &quot;Tove&quot;,&quot;Jani&quot;,&quot;Reminder&quot;,&quot;Don’t forget me this weekend!&quot; 6.1.3 JSON (JavaScript Object Notation) { &quot;to&quot;: &quot;Tove&quot;, &quot;from&quot;: &quot;Jani&quot;, &quot;heading&quot;: &quot;Reminder&quot;, &quot;body&quot;: &quot;Don’t forget me this weekend!&quot; } 6.1.4 YML or YAML (Yet Another Markup Language &gt; YAML Ain’t Markup Language) to: Tove from: Jani heading: Reminder body: Don’t forget me this weekend 6.1.5 BibTeX: most common bibliographic format We have already used this format in our lesson on sustainable writing. If you take a closer look at the record below, you may see that this format contains lots of valuable information. Most of this we will need for our project. @incollection{LuhmannKommunikation1982, title = {Kommunikation mit Zettelkiisten}, booktitle = {Öffentliche Meinung und sozialer Wandel: Für Elisabeth Noelle-Neumann = Public opinion and social change}, author = {Luhmann, Niklas}, editor = {Baier, Horst and Noelle-Neumann, Elisabeth}, date = {1982}, pages = {222--228}, publisher = {{westdt. Verl}}, location = {{Opladen}}, annotation = {OCLC: 256417947}, file = {Absolute/Path/To/PDF/Luhmann 1982 - Kommunikation mit Zettelkiisten.pdf}, isbn = {978-3-531-11533-7}, langid = {german} } 6.2 Larger Examples NB data example from here. There are some online converters that can help you to convert one format into another. For example: http://www.convertcsv.com/. 6.2.1 CSV / TSV city,growth_from_2000_to_2013,latitude,longitude,population,rank,state New York,4.8%,40.7127837,-74.0059413,8405837,1,New York Los Angeles,4.8%,34.0522342,-118.2436849,3884307,2,California Chicago,-6.1%,41.8781136,-87.6297982,2718782,3,Illinois TSV is a better option than a CSV, since TAB characters are very unlikely to appear in values. Neither TSV not CSV are good for preserving new line characters (\\n)—or, in other words, text split into multiple lines. As a workaround, one can convert \\n into some unlikely-to-occur character combination (for example, ;;;), which would allow to restore \\n later , if necessary. 6.2.2 JSON [ { &quot;city&quot;: &quot;New York&quot;, &quot;growth_from_2000_to_2013&quot;: &quot;4.8%&quot;, &quot;latitude&quot;: 40.7127837, &quot;longitude&quot;: -74.0059413, &quot;population&quot;: &quot;8405837&quot;, &quot;rank&quot;: &quot;1&quot;, &quot;state&quot;: &quot;New York&quot; }, { &quot;city&quot;: &quot;Los Angeles&quot;, &quot;growth_from_2000_to_2013&quot;: &quot;4.8%&quot;, &quot;latitude&quot;: 34.0522342, &quot;longitude&quot;: -118.2436849, &quot;population&quot;: &quot;3884307&quot;, &quot;rank&quot;: &quot;2&quot;, &quot;state&quot;: &quot;California&quot; }, { &quot;city&quot;: &quot;Chicago&quot;, &quot;growth_from_2000_to_2013&quot;: &quot;-6.1%&quot;, &quot;latitude&quot;: 41.8781136, &quot;longitude&quot;: -87.6297982, &quot;population&quot;: &quot;2718782&quot;, &quot;rank&quot;: &quot;3&quot;, &quot;state&quot;: &quot;Illinois&quot; } ] 6.2.3 YML/YAML YAML is often used only for a single set of parameters. city: New York growth_from_2000_to_2013: 4.8% latitude: 40.7127837 longitude: -74.0059413 population: 8405837 rank: 1 state: New York But it can also be used for storage of serialized data. It has advantages of both JSON and CSV: the overall simplicity of the format (no tricky syntax) is similar to that of CSV/TSV, but it is more readable than CSV/TSV in any text editor, and is more difficult to break—again, due to the simplicity of the format. New York: growth_from_2000_to_2013: 4.8% latitude: 40.7127837 longitude: -74.0059413 population: 8405837 rank: 1 state: New York Los Angeles: growth_from_2000_to_2013: 4.8% latitude: 34.0522342 longitude: -118.2436849 population: 3884307 rank: 2 state: California Chicago: growth_from_2000_to_2013: -6.1% latitude: 41.8781136 longitude: -87.6297982 population: 2718782 rank: 3 state: Illinois YAML files can be read with Python into dictionaries like so: import yaml dictionary = yaml.load(open(pathToFile)) You will most likely need to install yaml library; it is also quite easy to write a script that would read such serialized data. Note on installing libraries for python. In general, it should be as easy as running the following command in your command line tool: pip install --upgrade libraryName pip is the standard package installer for python; if you are running version 3.xx of python, it may be pip3 instead of pip. If you have Anaconda installed, you can also use Anacodnda interface to install packages; install is the command to install a package that you need; --upgrade is an optional argument that you would need only when you upgrade already installed package; libraryName is the name of the library that you want to install. This should work just fine, but sometimes it does not—usually when you have multiple versions of python installed and they may start conflicting with each other (another good reason to handle your python installations via Anaconda). There is, luckily, a workaround that seems to do the trick.You can modify your command in the following manner: python -m pip install --upgrade libraryName python here is whatever alias you are using for running python (e.g., in my case it is python3, so the full command will look: python3 -m pip install --upgrade libraryName) 6.3 In-Class Practice Let’s try to convert this bibTex file into other formats. Before we begin, however, let’s break down this task into smaller tasks and organize them together in some form of pseudocode. Which of the above-discussed formats would be most suitable? Why yes, why no? 6.4 Homework Take your bibliography in bibTeX format and convert it into: csv/tsv, json, and yaml; Hint: you should load your data into a dictionary; additionally, you might want to create (manually) a dictionary of bibTeX field: some of the fields are named differently, while they contain the same type of information — you want to identify those fields and unify them for your output format, which will improve the quality of your data (the process usually called normalization); hint: in order to figure out how to identify those fields, you may want to look into the Word Frequency program in Chapter 11. You can use this approach to identify all fields and count their frequencies, which will help you to determine which fields to keep and which to normalize (i.e., merge low-frequency fields into high-frequency fields). upload your results together with scripts to your homework github repository Python Work through Chapters 8 and 11 of Zelle’s book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 8 and 1-11 in Chapter 11; Watch Dr. Vierthaler’s videos: Episode 12: Functions Episode 13: Libraries and NLTK Episode 14: Regular Expressions Note: the sequences are somewhat different in Zelle’s textbook and Vierthaler’s videos. I would recommend you to always check Vierthaler’s videos and also check videos which cover topics that you read about in Zelle’s book. Webscraping (optional) if you are interested in webscraping, you can check the following tutorials: Milligan, Ian. 2012. “Automated Downloading with Wget.” Programming Historian, June. https://programminghistorian.org/lessons/automated-downloading-with-wget. Kurschinski, Kellen. 2013. “Applied Archival Downloading with Wget.” Programming Historian, September. https://programminghistorian.org/lessons/applied-archival-downloading-with-wget. Baxter, Richard. 2019. “How to download your website using WGET for Windows.” https://builtvisible.com/download-your-website-with-wget/. Alternatively, this operation can be done with a Python script: Turkel, William J., and Adam Crymble. 2012. “Downloading Web Pages with Python.” Programming Historian, July. https://programminghistorian.org/lessons/working-with-web-pages. Submitting homework: Homework assignment must be submitted by the beginning of the next class; Now, that you know how to use GitHub, you will be submitting your homework pushing it to github: Create a relevant subfoler in your HW070172 repository and place your HW files there; push them to your GitHub account; Email me the link to your repository with a short message (Something like: I have completed homework for Lesson 3, which is uploaded to my repository … in subfolder L03) In the subject of your email, please, add the following: CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber, where CCXXXXX is the numeric code of the course; LXX is the lesson for which the homework is being submitted; YourLastName is your last name, and YourMatriculationNumber is your matriculation number. 6.5 Homework Solution Before we proceed, let’s make sure that you have the same folder structure on you machine. This will help to ensure that we will not run into other issiues and can focus on solving one problem at a time. (Please, download these files for L06_Conversion folder: unzip and move them all into the folder). The structure should be as follows: . ├── MEMEX_SANDBOX │ └── data ├── L06_Conversion │ ├── comments.md │ ├── pseudocode.md │ ├── z_1_preliminary.py │ ├── z_2_conversion_simple.py │ ├── z_config.yml │ └── zotero_biblatex_sample.bib └── L07_Memex_Step1 ├── ... your scripts ... └── ... your scripts ... NB: ./MEMEX_SANDBOX/data/ is the target folder for all other assignments to follow. This is where we will be creating our memex. 6.5.1 Pseudocode solution look at the file, i.e. check the file in order to understand it structure create a holder for our data, which will be dictionary (dic, list, etc.) read as one big string split into records using \\n@ we will get a list of strings loop through all the records: NB: each records is a string that needs to be converted into something else. we need to split each record using ,\\n now we loop through the list of “key-value” pairs type{citationkey element: grab list element with index 0 (citationkey = record[0]) split the element on { recordType = element[0] citationKey = element[1] add a record into our dictionary using citationKey as a key value add recordType into the newly created record process the rest of the record: loop through the record, starting with 1: for r in record[1:]: split every element on = key = element[0].strip() value = element[1].strip() add our key-value pair into the dictionary Save dictionary into CSV, JSON, YAML 6.5.2 Scripts Script 1: analyzing bibTex data (z_1_preliminary.py) import os, yaml ########################################################### # VARIABLES ############################################### ########################################################### settingsFile = &quot;z_config.yml&quot; vars = yaml.load(open(settingsFile)) ########################################################### # FUNCTIONS ############################################### ########################################################### # analyze bibTeX data; identify what needs to be fixed def bibAnalyze(bibTexFile): tempDic = {} with open(bibTexFile, &quot;r&quot;, encoding=&quot;utf8&quot;) as f1: records = f1.read() records = records.split(&quot;\\n@&quot;) for record in records[1:]: # let process ONLY those records that have PDFs if &quot;.pdf&quot; in record.lower(): record = record.strip() record = record.split(&quot;\\n&quot;)[:-1] for r in record[1:]: r = r.split(&quot;=&quot;)[0].strip() if r in tempDic: tempDic[r] += 1 else: tempDic[r] = 1 results = [] for k,v in tempDic.items(): result = &quot;%010d\\t%s&quot; % (v, k) results.append(result) results = sorted(results, reverse=True) results = &quot;\\n&quot;.join(results) with open(&quot;bibtex_analysis.txt&quot;, &quot;w&quot;, encoding=&quot;utf8&quot;) as f9: f9.write(results) bibAnalyze(vars[&#39;bib_all&#39;]) This script will create the file bibtex_analysis.txt, which will be a frequency list of keys from all bibTeX records. We would want to convert this frequency list into a YML file which we can then load with yaml library (make sure to install it!). Loading yml data into a python dictionary is as easy as: dictionary = yaml.load(open(fileNameYml)). You can convert the frequency list into a proper yml file using regular expressions: Script 2: loading bibTeX data and converting to other formats (z_2_conversion_simple.py) import re import yaml &quot;&quot;&quot; 1. load bibtex file - bibliography should be curated in Zotero (one can program cleaning procedures into the script, but this is not as reliable); - loading bibtex data, keep only those records that have PDFs; - some processing might be necessary (like picking one file out of two and more) 2. convert into other formats - csv - json - yml &quot;&quot;&quot; ########################################################### # VARIABLES ############################################### ########################################################### settingsFile = &quot;z_config.yml&quot; settings = yaml.load(open(settingsFile)) bibKeys = yaml.load(open(&quot;zotero_biblatex_keys.yml&quot;)) ########################################################### # FUNCTIONS ############################################### ########################################################### # load bibTex Data into a dictionary def bibLoad(bibTexFile): bibDic = {} with open(bibTexFile, &quot;r&quot;, encoding=&quot;utf8&quot;) as f1: records = f1.read().split(&quot;\\n@&quot;) for record in records[1:]: # let process ONLY those records that have PDFs if &quot;.pdf&quot; in record.lower(): record = record.strip().split(&quot;\\n&quot;)[:-1] rType = record[0].split(&quot;{&quot;)[0].strip() rCite = record[0].split(&quot;{&quot;)[1].strip().replace(&quot;,&quot;, &quot;&quot;) bibDic[rCite] = {} bibDic[rCite][&quot;rCite&quot;] = rCite bibDic[rCite][&quot;rType&quot;] = rType for r in record[1:]: key = r.split(&quot;=&quot;)[0].strip() val = r.split(&quot;=&quot;)[1].strip() val = re.sub(&quot;^\\{|\\},?&quot;, &quot;&quot;, val) fixedKey = bibKeys[key] bibDic[rCite][fixedKey] = val print(&quot;=&quot;*80) print(&quot;NUMBER OF RECORDS IN BIBLIGORAPHY: %d&quot; % len(bibDic)) print(&quot;=&quot;*80) return(bibDic) ########################################################### # CONVERSION FUNCTIONS #################################### ########################################################### import json def convertToJSON(bibTexFile): data = bibLoad(bibTexFile) with open(bibTexFile.replace(&quot;.bib&quot;, &quot;.json&quot;), &#39;w&#39;, encoding=&#39;utf8&#39;) as f9: json.dump(data, f9, sort_keys=True, indent=4, ensure_ascii=False) import yaml def convertToYAML(bibTexFile): data = bibLoad(bibTexFile) with open(bibTexFile.replace(&quot;.bib&quot;, &quot;.yaml&quot;), &#39;w&#39;, encoding=&#39;utf8&#39;) as f9: yaml.dump(data, f9) # CSV is the trickest because bibTeX is not symmetrical def convertToCSV(bibTexFile): data = bibLoad(bibTexFile) # let&#39;s handpick fields that we want to save: citeKey, type, author, title, date headerList = [&#39;citeKey&#39;, &#39;type&#39;, &#39;author&#39;, &#39;title&#39;, &#39;date&#39;] header = &quot;\\t&quot;.join(headerList) dataNew = [header] for k,v in data.items(): citeKey = k if &#39;rType&#39; in v: rType = v[&#39;rType&#39;] else: rType = &quot;NA&quot; if &#39;author&#39; in v: author = v[&#39;author&#39;] else: author = &quot;NA&quot; if &#39;title&#39; in v: title = v[&#39;title&#39;] else: title = &quot;NA&quot; if &#39;date&#39; in v: date = v[&#39;date&#39;] else: date = &quot;NA&quot; tempVal = &quot;\\t&quot;.join([citeKey, rType, author, title, date]) dataNew.append(tempVal) finalData = &quot;\\n&quot;.join(dataNew) with open(bibTexFile.replace(&quot;.bib&quot;, &quot;.csv&quot;), &#39;w&#39;, encoding=&#39;utf8&#39;) as f9: f9.write(finalData) ########################################################### # RUN EVERYTHING ########################################## ########################################################### print(settings[&quot;bib_all&quot;]) #convertToJSON(settings[&quot;bib_all&quot;]) #convertToYAML(settings[&quot;bib_all&quot;]) #convertToCSV(settings[&quot;bib_all&quot;]) print(&quot;Done!&quot;) "],
["lesson-07.html", "7 Lesson 07 7.1 Building Memex - Step 1 7.2 Homework 7.3 Homework Solution", " 7 Lesson 07 7.1 Building Memex - Step 1 First and foremost, we need to build the structure for our memex. This involves the following steps: loading our bibTeX data and parsing it properly into a dictionary (you can build on the solution to the previous lesson); looping through the dictionary of our bibliographical data in order to save relevant information into the structure of our memex, which shoould be as shown on the screenshot below. Such folder structure is very easy to generate computationally knowing only the citation key of each publication, which will allow to manipulate our data with ease, no matter how massive it becomes. 7.1.1 Pseudocode load bibliographical data into a dictionary some data will need to be filtered, fixed, and probably reformatted slightly, etc. (build on the previously created function); use your Zotero data; loop through your dictionary and process each record: generate relative path for your record this path will consist of two parts: the path to your memex folder (either relative or absolute); this one you can define in your yml settings file (something like ./MEMEX_SANDBOX/data/). the relative path to a publucation within your memex folder the relative path must be generated from the citation key of each publication like this — ./1stLetter/2firstLetters/citeKey/ thus, if the key is McCartyHumanities2014, the path must be ./m/mc/McCartyHumanities2014/ (subFolder = key.lower()[0]; subSubFolder = key.lower()[:2]; subSubSubFolder = key; Question: Why would we need to .lower()?) create the subfolder, using this path save into this folder a bibTeX record for the publication with the name key.bib (i.e., McCartyHumanities2014.bib); copy the PDF file into this folder and save it in a similar manner: key.pdf (i.e., McCartyHumanities2014.pdf). 7.1.2 Code snippets Some of the following code snippets should be helpful (although you may find different ways of getting to the same results, which is, of course, totally fine). Snippet 1: Load YML File into a Dictionary import yaml settingsFile = &quot;z_config.yml&quot; settings = yaml.load(open(settingsFile)) Snippet 2: Create Folder, if Does Not Exist import os if not os.path.exists(folder): os.makedirs(folder) Snippet 3: Copy File to a Distination import shutil if not os.path.isfile(dst): shutil.copyfile(src, dst) where scr is the path to where the file was, and dst is the path to where you want your file. Keep in mind, that in dst you can use the name of the file that you want! Snippet 4: Making Path Properly As you remember from our introduction to command line, Mac and Windows format paths differently, which will create issues if manually construct the path. The best way to avoid any issues is to construct paths with a proper library. import os yourPath = os.path.join(folder, subfolder, subsubfolder, fileName) 7.2 Homework the assignment is described above additionally, take the solution scripts from the previous lesson and annotate every line of code; submit your annnotations together with the main assignment upload your results to your homework github repository 7.3 Homework Solution 7.3.1 Pseudocode creating the overall structure for our MEMEX on HDD using out bibliographical data, we algorithmically generate paths for each publication we create those paths we copy relevant data into those paths: bibliographical record (bib); PDF files Let’s break it down into smaller steps: first of all, it will actually be easier of take a look at this task from another perspective. we should start from a single publication, or, better, a single bibliographical record. after we figure out how to process a single publication, we will just need to figure out how to process all of them so, let’s start with a single bib record: (we have written a function before that creates a dictionary from every bib record (let’s call it loadBib()); so let our function take such a single-record dictionary as its argument); what will we need from our function (let’s call it processBibRecord())? it must do the following: to generate a unique path for the publication check if that path exists, if not: create it save bib record into that folder copy PDF to that folder what do we need to consider: we need a simple and transparent way to algorithmically generate all this data, which will ensure easy access, browsability, and overall transparency of our approach. In order to achieve it, we can use a single element from each record to generate all necessary paths and filenames. Namely, the citation key. path: our path will be a concatenation of the path to memex (for example, ./MEMEX_SANDBOX/data/) and a subpath unique to each publication, which we can generate from the citation key. for example, if the citation key is SavantMuslims2017, the publication path should be /s/sa/SavantMuslims2017/ the concatenated path thus will be: ./MEMEX_SANDBOX/data/s/sa/SavantMuslims2017/ NB: it may be a good idea to have a small function that takes paths to memex and the citation key as its arguments and returns the final path (let’s call it generatePublPath()). now, that we have the path, we can check whether it exists, and, if not, create it now, we have our path and the actual folder on HDD, it is easy-peasy-lemon-squeezy to save a bibliographical record and a PDF into that folder. for the same considerations of machine-readability, we should use use the citation key to name our files: for example, SavantMuslims2017.bib for our bibliographical record, and SavantMuslims2017.pdf for our PDF file. now, we have a function that does all that we need from a single record, we can write another function (let’s call it processAllRecords()) that will take the large bibliographical dictionary generated with loadBib() as its argument loops through this dictionary, and processes each record with processBibRecord(). 7.3.2 Note on Folder/File Organization It is crucial that all our files have their places and there are only as many files as we need. Otherwise, things get unmanageable very quickly. Create memex_sandbox repository on github add .gitignore with template for Python edit .gitignore on github.com, by adding the following two lines to the top _data/ .DS_Store Clone the repository to your machine Let’s organize everything nicely here. Like this: _bib/ _data/ _misc/ 1_build_structure.md 1_build_structure.py 2_OCR_test.py settings.yml _bib/ : all bibliography-related files Zotero bibliography language codes dictionary etc. _data/ : folder where we will be generating all the data; this folder is added to .gitignore so that none of the data would be uploaded to github (reason: copyright) _misc : all other miscellaneous files go here; we can also move all the files we no longer need into this folder (keeping the main folder clean). settings.yml should look something like: path_to_memex: ./_data/ bib_all: ./_bib/zotero_bibliography.bib bibtex_keys: ./_bib/bibtex_keys.yml 7.3.3 Script (1_build_structure.py) import os, shutil, re import yaml ########################################################### # VARIABLES ############################################### ########################################################### settingsFile = &quot;./settings.yml&quot; settings = yaml.load(open(settingsFile)) memexPath = settings[&quot;path_to_memex&quot;] ########################################################### # FUNCTIONS ############################################### ########################################################### # load bibTex Data into a dictionary def loadBib(bibTexFile): bibDic = {} with open(bibTexFile, &quot;r&quot;, encoding=&quot;utf8&quot;) as f1: records = f1.read().split(&quot;\\n@&quot;) for record in records[1:]: # let process ONLY those records that have PDFs if &quot;.pdf&quot; in record.lower(): completeRecord = &quot;\\n@&quot; + record record = record.strip().split(&quot;\\n&quot;)[:-1] rType = record[0].split(&quot;{&quot;)[0].strip() rCite = record[0].split(&quot;{&quot;)[1].strip().replace(&quot;,&quot;, &quot;&quot;) bibDic[rCite] = {} bibDic[rCite][&quot;rCite&quot;] = rCite bibDic[rCite][&quot;rType&quot;] = rType bibDic[rCite][&quot;complete&quot;] = completeRecord for r in record[1:]: key = r.split(&quot;=&quot;)[0].strip() val = r.split(&quot;=&quot;)[1].strip() val = re.sub(&quot;^\\{|\\},?&quot;, &quot;&quot;, val) bibDic[rCite][key] = val # fix the path to PDF if key == &quot;file&quot;: if &quot;;&quot; in val: #print(val) temp = val.split(&quot;;&quot;) for t in temp: if t.endswith(&quot;.pdf&quot;): val = t bibDic[rCite][key] = val print(&quot;=&quot;*80) print(&quot;NUMBER OF RECORDS IN BIBLIGORAPHY: %d&quot; % len(bibDic)) print(&quot;=&quot;*80) return(bibDic) # generate path from bibtex code, and create a folder, if does not exist; # if the code is `SavantMuslims2017`, the path will be pathToMemex+`/s/sa/SavantMuslims2017/` def generatePublPath(pathToMemex, bibTexCode): temp = bibTexCode.lower() directory = os.path.join(pathToMemex, temp[0], temp[:2], bibTexCode) return(directory) # process a single bibliographical record: 1) create its unique path; 2) save a bib file; 3) save PDF file def processBibRecord(pathToMemex, bibRecDict): tempPath = generatePublPath(pathToMemex, bibRecDict[&quot;rCite&quot;]) print(&quot;=&quot;*80) print(&quot;%s :: %s&quot; % (bibRecDict[&quot;rCite&quot;], tempPath)) print(&quot;=&quot;*80) if not os.path.exists(tempPath): os.makedirs(tempPath) bibFilePath = os.path.join(tempPath, &quot;%s.bib&quot; % bibRecDict[&quot;rCite&quot;]) with open(bibFilePath, &quot;w&quot;, encoding=&quot;utf8&quot;) as f9: f9.write(bibRecDict[&quot;complete&quot;]) pdfFileSRC = bibRecDict[&quot;file&quot;] pdfFileDST = os.path.join(tempPath, &quot;%s.pdf&quot; % bibRecDict[&quot;rCite&quot;]) if not os.path.isfile(pdfFileDST): # this is to avoid copying that had been already copied. shutil.copyfile(pdfFileSRC, pdfFileDST) ########################################################### # PROCESS ALL RECORDS ##################################### ########################################################### def processAllRecords(bibData): for k,v in bibData.items(): processBibRecord(memexPath, v) bibData = loadBib(settings[&quot;bib_all&quot;]) processAllRecords(bibData) print(&quot;Done!&quot;) "],
["lesson-08.html", "8 Lesson 08 8.1 Building Memex - Step 2 8.2 OCR software: Tesseract 8.3 Task: General Map Pseudocode 8.4 Code snippets &amp; functions 8.5 Clean Temporary Copy of a PDF 8.6 One more note on OCR 8.7 Homework 8.8 Homework Solution 8.9 Tesseract Language Codes", " 8 Lesson 08 8.1 Building Memex - Step 2 Now that we have all files and foolders created, we need to extract text from PDFs. We can use OCR, or (optical character recognition), — a method that allows us to extract editable text from images containing text. 8.2 OCR software: Tesseract In order to run OCR, we need to have a suitable software installed on our computers. Tesseract is one of such options—it is free and open source OCR engine. We will need to install Tesseract and some Python libraries that “cooperate” with Tesseract. Keep in mind that usually OCR is language specific, i.e. you also need to have additional files for relevant languages installed; and when you run an OCR procedure, you need to explicitely declare the language that you want to recognize (details below). Language codes used in Tesseract can be found at the end of this lesson (you can also find suggestions on how to deal with automatic language selection in your Python code). General information on how to make Tesseract work on your machine can be found here. 8.2.0.1 Installing on Mac We can use brew to install everything we need on Mac: brew install tesseract Support for additional languages can be installed in the following manner (lang is the language code that Tesseract uses to refer to a specific language): brew install tesseract-lang The following command will list all available languages (as language codes). This will work only after you have installed Tesseract. tesseract --list-langs 8.2.0.2 Installing on Linux sudo apt install tesseract-ocr sudo apt install libtesseract-dev On how to install additional languages, see here. 8.2.0.3 Installing on Windows (Source: https://automaticaddison.com/how-to-set-up-anaconda-for-windows-10/) Go to Tesseract at UB Mannheim. Download the Tesseract installer (64-bit). Run the installe and follow the the prompts. (You can choose additional languages during the installation.) Once Tesseract OCR is downloaded and installed, find it on your system. Copy the name of the folder where it is located. The default path is C:\\Program Files\\Tesseract-OCR Search for “Environment Variables” on your computer. (“Environment Variable” is a button in “System Properties”, Tab “Advanced”) Under “System Variables,” select “Path”, and then click “Edit”. Add the path: C:\\Program Files\\Tesseract-OCR (or whatever it is on you machine) Click “OK” a few times to close all windows. Open up the Anaconda Prompt or Anaconda Powershell Prompt Type tesseract and hit “Enter”. If everything is installed correctly, you should see: Usage: tesseract --help | --help-extra | --version tesseract --list-langs tesseract imagename outputbase [options...] [configfile...] OCR options: -l LANG[+LANG] Specify language(s) used for OCR. NOTE: These options must occur before any configfile. Single options: --help Show this help message. --help-extra Show extra help for advanced users. --version Show version information. --list-langs List available languages for tesseract engine. 8.2.0.4 For all systems after Tesserract is installed After Tesseract is installed we need to install a couple of Python libraries: python -m pip install Pillow python -m pip install pytesseract Just remember that you need to use the Python command that you are using to run your scripts (for example, on Mac python is for Python 2.7, and python3 is commonly for Python 3.X) Alternatively, if you use Anaconda: conda install -c conda-forge Pillow conda install -c conda-forge pytesseract 8.2.0.5 Final Test In you command line tool, type tesseract. If everything is installed correctly, you shoudl see the following: user % tesseract Usage: tesseract --help | --help-extra | --version tesseract --list-langs tesseract imagename outputbase [options...] [configfile...] OCR options: -l LANG[+LANG] Specify language(s) used for OCR. NOTE: These options must occur before any configfile. Single options: --help Show this help message. --help-extra Show extra help for advanced users. --version Show version information. --list-langs List available languages for tesseract engine. 8.3 Task: General Map Pseudocode Like in the previous lesson, let’s start with processing a single publication (using a dictionary with the data on a single record). We want to process PDF page by page, save an image of each page, extract text from every page and save the extracted text into some format that would keep the entire text of our publication, preserving separation into pages (Hint: dictionary &gt; json). After we can do that, we will need to write a little bit of code that will process all our publicationa. What we want to have in the end is each and every publication in our memex processed this way. For consistency, let’s call this script 2_OCR.py 8.4 Code snippets &amp; functions 8.4.1 New libraries We will need the following new libraries that you will need to install, if you have not already: pdf2image # extracts images from PDF pytesseract # interacts with Tesseract, which extracts text from images PyPDF2 # cleans PDFs 8.4.2 Code Reuse We have already discussed functions as a useful mechanism for code reuse; additionally, use can place often-used functions into a separate file (let’s call it functions.py) and import it with import functions in our other script. (IMPORTANT: functions.py must be in the same folder as you other script that imports it). For example: ############################# # STORING FUNCTIONS ######### ############################# import os # generate path from bibtex code: def generatePublPath(pathToMemex, bibTexCode): temp = bibTexCode.lower() directory = os.path.join(pathToMemex, temp[0], temp[:2], bibTexCode) return(directory) ############################# # REUSING FUNCTIONS ######### ############################# import functions pathToMemex = &quot;pathToMemex&quot; citationKey = &quot;Bush_AsWeMayThink_1945&quot; publPath = functions.generatePublPath(pathToMemex, citationKey) NB: In general, you can also package your functions into a library and use it as a regular library. For more details, check Paul Vierthaler’s “Studium Digitale” Series (particularly Video 5). 8.5 Clean Temporary Copy of a PDF Why do we need this function? You may have PDFs that you have already annotated with highlights, comments, etc. Highlights will interfere with the OCR process and will most likely be omitted from the final results. For this reason, we will want to clean all PDFs before we OCR them. In order not to delete your annotations, we will generate a temporary PDF file, which we can safely delete after processing it. The functions take a path to a PDF as its argument; creates a clean copy of it and saves it with a different name, replacing the suffix .pdf with _TEMP.pdf; and then returns the path to this temporary PDF. We can easily process it and delete after completion. import PyPDF2 def removeCommentsFromPDF(pathToPdf): with open(pathToPdf, &#39;rb&#39;) as pdf_obj: pdf = PyPDF2.PdfFileReader(pdf_obj) out = PyPDF2.PdfFileWriter() for page in pdf.pages: out.addPage(page) out.removeLinks() tempPDF = pathToPdf.replace(&quot;.pdf&quot;, &quot;_TEMP.pdf&quot;) with open(tempPDF, &#39;wb&#39;) as f: out.write(f) return(tempPDF) 8.5.1 OCR-ing PDF The following function takes three arguments: path to memex, citation key, and the language of the publication. The function processes PDFs: extracts images, OCRs them, collects results into a dictionary, saves OCR results into a JSON file. import os, json import pdf2image, pytesseract def ocrPublication(pathToMemex, citationKey, language): publPath = functions.generatePublPath(pathToMemex, citationKey) pdfFile = os.path.join(publPath, citationKey + &quot;.pdf&quot;) jsonFile = os.path.join(publPath, citationKey + &quot;.json&quot;) saveToPath = os.path.join(publPath, &quot;pages&quot;) pdfFileTemp = removeCommentsFromPDF(pdfFile) if not os.path.isfile(jsonFile): if not os.path.exists(saveToPath): os.makedirs(saveToPath) print(&quot;\\t&gt;&gt;&gt; OCR-ing: %s&quot; % citationKey) textResults = {} images = pdf2image.convert_from_path(pdfFileTemp) pageTotal = len(images) pageCount = 1 for image in images: image = image.convert(&#39;1&#39;) finalPath = os.path.join(saveToPath, &quot;%04d.png&quot; % pageCount) image.save(finalPath, optimize=True, quality=10) text = pytesseract.image_to_string(image, lang=language) textResults[&quot;%04d&quot; % pageCount] = text print(&quot;\\t\\t%04d/%04d pages&quot; % (pageCount, pageTotal)) pageCount += 1 with open(jsonFile, &#39;w&#39;, encoding=&#39;utf8&#39;) as f9: json.dump(textResults, f9, sort_keys=True, indent=4, ensure_ascii=False) else: print(&quot;\\t&gt;&gt;&gt; %s has already been OCR-ed...&quot; % citationKey) os.remove(pdfFileTemp) 8.5.2 Missing functions You can reuse these functions to assemble the entire process needed for this lesson. You will need to write a couple of your own though: first, you will need to solve the problem of picking the right language (see my comments below in Tesseract Language Codes) you will need to write the final block that processes all PDFs. 8.6 One more note on OCR Unfortunately, OCR-ing is a rather slow process, taking 10-30 seconds per page (depending on the speed of your computer). So, you will have to run it for some time in order to process everything—the more you have the longer it will take to process all. Good news is that this operation needs to be done only once per publication. Additionally, we can use some tricks to speed up the process, which we will discuss next time. 8.7 Homework the task is described above. additionally, take the solution scripts from the previous lesson and annotate every line of code; submit your annotations together with the main assignment; if you have any suggestions for improvements, please share them (this will count as extra points :). upload your results to your memex github repository place annotated scripts into _misc subfolder 8.8 Homework Solution Note: all this new code is available here as well: https://github.com/maximromanov/memex_sandbox First, a little update: I have added a function that will load settings from our YAML file. The main goals here are twofold: first, to avoid using YAML libraries (as we discovered, they function somewhat differently and one is not compartible with the latest versions of Python); second, to add a bit of flexibility to our settings file, which now can include annotations/comments that YAML files do not exactly support. Here is an example of our new settings file: # !!! DO NOT CHANGE EXISTING KEYS !!! ONLY VALUES # PATH TO MEMEX path_to_memex: ./_data/ # BIBLIOGRAPHICAL FILES bib_all: ./_bib/zotero_bibliography.bib bibtex_keys: ./_bib/bibtex_keys.yml language_keys: ./_bib/bibtex_languages.yml # TEMPLATES AND CONTENT FILES FOR THE INTERFACE template_page: ./_misc/template_page.html template_index: ./_misc/template_index.html content_index: ./_misc/content_index.html # GENERATED FILES AND EXTENSIONS memexDF_file: ./_data/memex.documentFrequencies tfPubl_ext: .publTF # term frequencies per entire publication tfPage_ext: .pageTF # term frequencies per each page tfIdfPubl_ext: .publTFIDF # tfIdf frequencies per entire publication tfIdfPage_ext: .pageTFIDF # tfIdf frequencies per each page In this file you can add comments (starting with #); you can visually separate blocks of variables so that they are more readable. Make sure not to change the keys — only the values may be changed. Our loading function looks as shown in the block below. Essentially, it does the following: first, it removes all non-YAML elements; second, it splits the cleaned text into units that contain key-value pairs; and, third, it splits these units into keys and values and adds them into a dictionary, which is then “returned”. def loadYmlSettings(ymlFile): with open(ymlFile, &quot;r&quot;, encoding=&quot;utf8&quot;) as f1: data = f1.read() data = re.sub(r&quot;#.*&quot;, &quot;&quot;, data) # remove comments data = re.sub(r&quot;\\n+&quot;, &quot;\\n&quot;, data) # remove extra linebreaks used for readability data = re.split(r&quot;\\n(?=\\w)&quot;, data) # splitting dic = {} for d in data: if &quot;:&quot; in d: d = re.sub(r&quot;\\s+&quot;, &quot; &quot;, d.strip()) d = re.split(r&quot;^([^:]+) *:&quot;, d)[1:] key = d[0].strip() value = d[1].strip() dic[key] = value return(dic) The main OCR-ing Script: # NEW LIBRARIES import pdf2image import pytesseract import os, yaml, json, random import functions ########################################################### # VARIABLES ############################################### ########################################################### settingsFile = &quot;settings.yml&quot; settings = yaml.load(open(settingsFile)) memexPath = settings[&quot;path_to_memex&quot;] langKeys = yaml.load(open(settings[&quot;language_keys&quot;])) ########################################################### # TRICKY FUNCTIONS ######################################## ########################################################### def ocrPublication(pathToMemex, citationKey, language): publPath = functions.generatePublPath(pathToMemex, citationKey) pdfFile = os.path.join(publPath, citationKey + &quot;.pdf&quot;) jsonFile = os.path.join(publPath, citationKey + &quot;.json&quot;) saveToPath = os.path.join(publPath, &quot;pages&quot;) if not os.path.isfile(jsonFile): if not os.path.exists(saveToPath): os.makedirs(saveToPath) print(&quot;\\t&gt;&gt;&gt; OCR-ing: %s&quot; % citationKey) textResults = {} images = pdf2image.convert_from_path(pdfFile) pageTotal = len(images) pageCount = 1 for image in images: text = pytesseract.image_to_string(image, lang=language) textResults[&quot;%04d&quot; % pageCount] = text image = image.convert(&#39;1&#39;) # binarizes image, reducing its size finalPath = os.path.join(saveToPath, &quot;%04d.png&quot; % pageCount) image.save(finalPath, optimize=True, quality=10) print(&quot;\\t\\t%04d/%04d pages&quot; % (pageCount, pageTotal)) pageCount += 1 with open(jsonFile, &#39;w&#39;, encoding=&#39;utf8&#39;) as f9: json.dump(textResults, f9, sort_keys=True, indent=4, ensure_ascii=False) else: print(&quot;\\t&gt;&gt;&gt; %s has already been OCR-ed...&quot; % citationKey) def identifyLanguage(bibRecDict, fallBackLanguage): if &quot;langid&quot; in bibRecDict: try: language = langKeys[bibRecDict[&quot;langid&quot;]] message = &quot;\\t&gt;&gt; Language has been successfuly identified: %s&quot; % language except: message = &quot;\\t&gt;&gt; Language ID `%s` cannot be understood by Tesseract; fix it and retry\\n&quot; % bibRecDict[&quot;langid&quot;] message += &quot;\\t&gt;&gt; For now, trying `%s`...&quot; % fallBackLanguage language = fallBackLanguage else: message = &quot;\\t&gt;&gt; No data on the language of the publication&quot; message += &quot;\\t&gt;&gt; For now, trying `%s`...&quot; % fallBackLanguage language = fallBackLanguage print(message) return(language) ########################################################### # PROCESS ALL RECORDS: APPROACH 2 ######################### ########################################################### # Why this way? Our computers are now quite powerful; they # often have multiple cores and we can take advantage of this; # if we process our data in the manner coded below --- we shuffle # our publications and process them in random order --- we can # run multiple instances fo the same script and data will # be produced in parallel. You can run as many instances as # your machine allows (you need to check how many cores # your machine has). Even running two scripts will cut # processing time roughly in half. def processAllRecords(bibData): keys = list(bibData.keys()) random.shuffle(keys) for key in keys: bibRecord = bibData[key] functions.processBibRecord(memexPath, bibRecord) language = identifyLanguage(bibRecord, &quot;eng&quot;) ocrPublication(memexPath, bibRecord[&quot;rCite&quot;], language) bibData = functions.loadBib(settings[&quot;bib_all&quot;]) processAllRecords(bibData) 8.9 Tesseract Language Codes As you can see below there are quite a few languages. You may need only those languages that you can read. The question is how to determine the language of a publication? Usually, when you add data to Zotero, there will be a field that states the language. There are two problems, however. First, the language field is often empty. Second, the same language can be indicated in a variety of ways (for example, en, eng, English, Englisch, etc.). Tesseract uses triliteral ISO 639-2 standard for language codes (eng for English). How to resolve this issue? There are three things to consider: you can curate you Zotero data, manually fixing language codes; you can create a dictionary (yaml file) that will have most common spellings of languages that occure in your data (you can build a frequency list in order to identify them; then convert that data into YAML format with regular expressions; then manually fix all irregularities); you can write a function that checks if the language code in your data corresponds to any code in Tesseract: if true, the function will return it; if not, it defaults to the most common language in your library. In general, the most optimal approach will have elements of all three. user % tesseract --list-langs List of available languages (162): afr amh ara asm aze aze_cyrl bel ben bod bos bre bul cat ceb ces chi_sim chi_sim_vert chi_tra chi_tra_vert chr cos cym dan deu div dzo ell eng enm epo est eus fao fas fil fin fra frk frm fry gla gle glg grc guj hat heb hin hrv hun hye iku ind isl ita ita_old jav jpn jpn_vert kan kat kat_old kaz khm kir kmr kor kor_vert lao lat lav lit ltz mal mar mkd mlt mon mri msa mya nep nld nor oci ori osd pan pol por pus que ron rus san script/Arabic script/Armenian script/Bengali script/Canadian_Aboriginal script/Cherokee script/Cyrillic script/Devanagari script/Ethiopic script/Fraktur script/Georgian script/Greek script/Gujarati script/Gurmukhi script/HanS script/HanS_vert script/HanT script/HanT_vert script/Hangul script/Hangul_vert script/Hebrew script/Japanese script/Japanese_vert script/Kannada script/Khmer script/Lao script/Latin script/Malayalam script/Myanmar script/Oriya script/Sinhala script/Syriac script/Tamil script/Telugu script/Thaana script/Thai script/Tibetan script/Vietnamese sin slk slv snd snum spa spa_old sqi srp srp_latn sun swa swe syr tam tat tel tgk tha tir ton tur uig ukr urd uzb uzb_cyrl vie yid yor "],
["lesson-09.html", "9 Lesson 09 9.1 Building Memex - Step 3 9.2 Code snippets &amp; functions 9.3 Homework 9.4 Homework Solution 9.5 Additional materials: HTML, CSS; TFIDF", " 9 Lesson 09 9.1 Building Memex - Step 3 Now, as we have images of pages extracted and OCRed, we may want to create some intefrace that would allow us to move around our memex and do rather traditional reading of publications in our library. Perhaps the easiest way for building a simple interface would be to use HTML, which allows to easily link everything together and can be easily used in any browser. What do we need to do? We want a function that creates interconnected HTML pages for each publication—with all the relevant bibliographical information included into each and every page. The working function is actually given below: your main task here will be to figure out how it works; and to write a bit of code that processes every publication in your memex (essentially, you will need to slightly modify a “processAll” function from previous assignments). Your other, more difficult task will be to generate a starting page and a page that lists all publications in your memex and allows one to navigate from there to any of listed publications. Details are given below. Keep reading :) (You do not really need to know any HTML to complete this assignment, but, just in case you are curious, you can find links to relevant materials in Additional Materials below). 9.2 Code snippets &amp; functions 9.2.1 The main function: Publication Interface Here is one, big, scary function that merges together page images, OCR-ed text, and bibliographical information into a simple HTML-based interface. While it looks long and scary, it is actually quite simple — most of the code is simple fine/replace operations that are populating a template with relevant information. In a nutshell, the function: takes a citation key and the path to a relevant .bib file as its arguments loads: OCRed data bibliographical information an HTML template loops through the pages of OCRed data and inserts relevant information into relevant slots saves each page into its place # generate interface for the publication def generatePublicationInterface(citeKey, pathToBibFile): print(&quot;=&quot;*80) print(citeKey) jsonFile = pathToBibFile.replace(&quot;.bib&quot;, &quot;.json&quot;) with open(jsonFile) as jsonData: ocred = json.load(jsonData) pNums = ocred.keys() pageDic = functions.generatePageLinks(pNums) # load page template with open(settings[&quot;template_page&quot;], &quot;r&quot;, encoding=&quot;utf8&quot;) as ft: template = ft.read() # load individual bib record bibFile = pathToBibFile bibDic = functions.loadBib(bibFile) bibForHTML = functions.prettifyBib(bibDic[citeKey][&quot;complete&quot;]) orderedPages = list(pageDic.keys()) for o in range(0, len(orderedPages)): #print(o) k = orderedPages[o] v = pageDic[orderedPages[o]] pageTemp = template pageTemp = pageTemp.replace(&quot;@PAGELINKS@&quot;, v) pageTemp = pageTemp.replace(&quot;@PATHTOFILE@&quot;, &quot;&quot;) pageTemp = pageTemp.replace(&quot;@CITATIONKEY@&quot;, citeKey) if k != &quot;DETAILS&quot;: mainElement = &#39;&lt;img src=&quot;@PAGEFILE@&quot; width=&quot;100%&quot; alt=&quot;&quot;&gt;&#39;.replace(&quot;@PAGEFILE@&quot;, &quot;%s.png&quot; % k) pageTemp = pageTemp.replace(&quot;@MAINELEMENT@&quot;, mainElement) pageTemp = pageTemp.replace(&quot;@OCREDCONTENT@&quot;, ocred[k].replace(&quot;\\n&quot;, &quot;&lt;br&gt;&quot;)) else: mainElement = bibForHTML.replace(&quot;\\n&quot;, &quot;&lt;br&gt; &quot;) mainElement = &#39;&lt;div class=&quot;bib&quot;&gt;%s&lt;/div&gt;&#39; % mainElement mainElement += &#39;\\n&lt;img src=&quot;wordcloud.jpg&quot; width=&quot;100%&quot; alt=&quot;wordcloud&quot;&gt;&#39; pageTemp = pageTemp.replace(&quot;@MAINELEMENT@&quot;, mainElement) pageTemp = pageTemp.replace(&quot;@OCREDCONTENT@&quot;, &quot;&quot;) # @NEXTPAGEHTML@ and @PREVIOUSPAGEHTML@ if k == &quot;DETAILS&quot;: nextPage = &quot;0001.html&quot; prevPage = &quot;&quot; elif k == &quot;0001&quot;: nextPage = &quot;0002.html&quot; prevPage = &quot;DETAILS.html&quot; elif o == len(orderedPages)-1: nextPage = &quot;&quot; prevPage = orderedPages[o-1] + &quot;.html&quot; else: nextPage = orderedPages[o+1] + &quot;.html&quot; prevPage = orderedPages[o-1] + &quot;.html&quot; pageTemp = pageTemp.replace(&quot;@NEXTPAGEHTML@&quot;, nextPage) pageTemp = pageTemp.replace(&quot;@PREVIOUSPAGEHTML@&quot;, prevPage) pagePath = os.path.join(pathToBibFile.replace(citeKey+&quot;.bib&quot;, &quot;&quot;), &quot;pages&quot;, &quot;%s.html&quot; % k) with open(pagePath, &quot;w&quot;, encoding=&quot;utf8&quot;) as f9: f9.write(pageTemp) 9.2.2 Additional functions As we discussed before, sometimes it makes sense to move some of the code into separate functions. These are such functions that I removed from the main code so that it is more readable (although a few more functions can be also extracted from the main code). TOC Links: The following function generate links to all pages in a given publication so that it is easier to navigate and move around; each page in a publication recieves its own list of links where the current page is colored with red. def generatePageLinks(pNumList): listMod = [&quot;DETAILS&quot;] listMod.extend(pNumList) toc = [] for l in listMod: toc.append(&#39;&lt;a href=&quot;%s.html&quot;&gt;%s&lt;/a&gt;&#39; % (l, l)) toc = &quot; &quot;.join(toc) pageDic = {} for l in listMod: pageDic[l] = toc.replace(&#39;&gt;%s&lt;&#39; % l, &#39; style=&quot;color: red;&quot;&gt;%s&lt;&#39; % l) return(pageDic) HTML-Friendly BIB: The following function simply makes a bib record look more readable, more HTML friendly. It removes excessive curly brackets; and some field that are not needed for display (you can modify it to adjust the way your records look). def prettifyBib(bibText): bibText = bibText.replace(&quot;{{&quot;, &quot;&quot;).replace(&quot;}}&quot;, &quot;&quot;) bibText = re.sub(r&quot;\\n\\s+file = [^\\n]+&quot;, &quot;&quot;, bibText) bibText = re.sub(r&quot;\\n\\s+abstract = [^\\n]+&quot;, &quot;&quot;, bibText) return(bibText) 9.2.3 Extra function It always makes sense to write functions that do operations that we need frequently. For example, the fucntion below generates a dictionary of citation keys and paths to specific types of files. For instance, we can quickly create a dictionary of .bib files and then use this dictionary to process all .bib files. def dicOfRelevantFiles(pathToMemex, extension): dic = {} for subdir, dirs, files in os.walk(pathToMemex): for file in files: # process publication tf data if file.endswith(extension): key = file.replace(extension, &quot;&quot;) value = os.path.join(subdir, file) dic[key] = value return(dic) Additional value: in the first two steps we used bibTex bibliography exported from Zotero to process our publications and OCR relevant PDF files. Technically, right after we generated the structure (created all relevant folders, copied PDFs, and generated individual .bib files), we no longer need to rely on our big bibliography file, but rather use the very structure of our memex. The advantage of such an approach is that we can merge multiple memexes and then run only operations that are necessary to connect new publications with old ones. 9.2.4 Code Reuse We have already discussed functions as a useful mechanism for code reuse; additionally, use can place often-used functions into a separate file (let’s call it functions.py) and import it with import functions in our other script. (IMPORTANT: functions.py must be in the same folder as you other script that imports it). 9.2.5 Missing functions: The Index Page and the Contents Page Since we are essentially creating a local website, we need a starting page—index.html—which will serve as the main entry point for our memex. Additionally, we would need a page where all the publications of our memex will be listed so that we could go and read whatever we are interested in. Your main task is to write such a function. In general, the main function avobe is your guide, although these two pages are significantly simpler. For the index page you need to “join” the index page template with the index page content (both are prepared and available in ./_misc/ folder). The content page is a bit trickier — you need to generate a list of publications—with links—and “join” it with the index page template. Your content page may look something like this: For information for each publication, you should use (at least): author or editor, year, title. You can use the following snippet as an example, but you are more than welcome to come us with your own representation. &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;a/ab/AbbottThat2019/pages/DETAILS.html&quot;&gt;[AbbottThat2019]&lt;/a&gt; Abbott, Nicholas (2019) - &lt;i&gt;“In That One the Ālif Is Missing”: Eunuchs and the Politics of Masculinity in Early Colonial North India&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;a/ab/AbdullahMandaean2018/pages/DETAILS.html&quot;&gt;[AbdullahMandaean2018]&lt;/a&gt; Abdullah, Thabit A. J. (2018) - &lt;i&gt;The Mandaean Community and Ottoman-British Rivalry in Late 19th-Century Iraq: The Curious Case of Shaykh Ṣaḥan&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;a/ab/AbdurasulovMaking2020/pages/DETAILS.html&quot;&gt;[AbdurasulovMaking2020]&lt;/a&gt; Abdurasulov, Ulfatbek (2020) - &lt;i&gt;Making Sense of Central Asia in Pre-Petrine Russia&lt;/i&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;a/ac/Ackerman-LiebermanContractual2011/pages/DETAILS.html&quot;&gt;[Ackerman-LiebermanContractual2011]&lt;/a&gt; Ackerman-Lieberman, Phillip I. (2011) - &lt;i&gt;Contractual Partnerships in the Geniza and the Relationship between Islamic Law and Practice&lt;/i&gt;&lt;/li&gt; &lt;/ul&gt; To make it a little bit easier, the actual template for each publication may look as follows: &lt;li&gt;&lt;a href=&quot;@PATHTOPUBL@/pages/DETAILS.html&quot;&gt;[@CITEKEY@]&lt;/a&gt; @AUTHOR@ (@DATE@) - &lt;i&gt;@TITLE@&lt;/i&gt;&lt;/li&gt; 9.3 Homework the task is described above. additionally, take the solution scripts from the previous lesson and annotate every line of code; submit your annotations together with the main assignment; if you have any suggestions for improvements, please share them (this will count as extra points :). upload your results to your memex github repository place annotated scripts into _misc subfolder 9.4 Homework Solution Check the script 3_Interface1.py in the memex_sandbox repository (https://github.com/maximromanov/memex_sandbox). 9.5 Additional materials: HTML, CSS; TFIDF To get a better idea of HTML and CSS, take a look at the following tutorials at Programming Historian: (Turkel and Crymble 2012b) and (Turkel and Crymble 2012a); see also starting with HTML + CSS. Next time we will discuss keyword extraction. It will be beneficial for all if you do some readings on TF-IDF. Please, read Chapter 1 in Ramsay’s Reading Machines (Ramsay 2011), which is digitally available via Uni Wien Library; and take a quick look at Lavin’s Analyzing Documents with TF-IDF on Programming Historian (Lavin 2019). References "],
["lesson-10.html", "10 Lesson 10 10.1 Building Memex - Step 4 10.2 Some explanations: TF-IDF, Document Distance 10.3 Practicalities 10.4 Code Snippets 10.5 Homework 10.6 Homework Solution", " 10 Lesson 10 10.1 Building Memex - Step 4 One of the most interesting aspects behind the idea of memex is connections among its texts: the network of algorithmically generated connections that would suggest one what else s/he should read or might be interested to read, based on what s/he is reading right now. There are plenty of different methods that are actively used in digital humanities to identify different kinds of similarities among texts in a corpus, a given body of texts. These methods may be used to identify duplicate or near-duplicate texts (document distance), to identify texts that cover similar themes and topics (topic modeling: for example, Blei 2012), to identify texts that belong to the same forms and genres, to identify texts that might have been written by the same individual (stylometric analysis: for example, Eder, Rybicki, and Kestemont 2016), etc. What is common among all these methods is that they mathematically manipulate numeric abstractions of texts: first, texts are reduced to frequency lists of different kinds; then they are mathematically compared with each other in order to identify different types of similarities. What is different among all these methods is how the numeric abstraction is generated, what features are selected for comparison, and what kind of formulas/algorithms are used to generate similarity measures. For our purpose, which would be finding thematically similar texts and sections of texts, we would benefit the most from using a combination of the tf-idf (term-frequency - inverse document frequency) method and a document distance algorithm. Chapter 1 of Ramsay’s Reading Machines (Ramsay 2011), which is digitally available via Uni Wien Library, gives a nice general overview of the approach. Matthew Lavin’s Lavin’s Analyzing Documents with TF-IDF on Programming Historian (Lavin 2019) provides a detailed technical description of the entire approach. Please, make sure to read these. 10.2 Some explanations: TF-IDF, Document Distance To quickly sum up, the steps will be as follows: [TF-IDF] We calculate term frequencies of all terms in all documents (i.e., our OCR results). Term frequency is relative frequency, i.e. absolute frequency of a term divided by the overall number of terms in a document. We then calculate inverse document frequency, computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears. We then calculate tf-idf as a product of tf and idf values (sometimes additional steps are included to normalize results). [Filtering] before document distance is calculated, some filtering might be desirable. Filtering is useful and sometimes necessary for a number of reasons. For example, if we are looking for similarities between texts, we may want to exclude terms that occur only in a very small number of texts (additionally, considering that we work with imperfect OCR results, most terms with df=1 are likely to be incorrectly recognized words). Additionally, it also makes sense to filter out terms that occur in too many documents (those terms that occur in all documents would carry no relevant value). An important practical benefit of such filtering is that we will reduce the amount of data that will be used for calculations, and therefore less resources will be required for calculations and they will be done faster. Additionally, filtering can be done by using language-specific lists of stopwords (for example, there is no need to include such English words as “a”, “the”, “and”, “or” into calculations). You can find lists of stopwords for specific languages online. Many relevant python packages will have lists of stopwords already included. Curating your own lists of stopwords is usually the best practice. Some other procedures can be used to improve results. For example stemming or, better, lemmatization will help to reduce morphological complexity of texts, which is particularly important for highly inflective and agglutinative languages. (however, stemming will also render results less readable). There are two caveats to consider before using these methods: first of all, both methods are language specific — in other words, for example, if you accidentaly apply English lemmatization to German (or the other way around), results will most likely be horrible; second, OCR results are not perfect: again, OCRing German text using eng as a language parameter in Tesseract may yield decent results, but all characters with umlauts are likely to be replaced with corresponding English characters; the results of applying lemmatization of stemming to such text will generate more errors. That said, one should still experiment with these methods. [Document Distance Calculations] last step: we apply some distance measure calcucations to each and every pair of documents/publications (which are now represented as numeric vectors). Practical Example. Let’s take a look at a very simple example of comparing the following simple sentences using our approach: President Obama returned from his trip to Europe. President Bush returned from his trip to China. The president returned from his trip. And now about something completely different. As we discussed, we need to convert each text (sentence) into a frequency list. For calculations each text (sentence) is vectorized, i.e. frequencies of each word of each sentence are mapped on the entire vocabulary of the corpus (i.e., all analyzed texts). If any specific word is missing in a particilar text (sentence), its frequency is 0. This is shown in Table 10.1 below. Table 10.1: Vectorized texts 1 2 3 4 about 0 0 0 1 and 0 0 0 1 bush 0 1 0 0 china 0 1 0 0 completely 0 0 0 1 different 0 0 0 1 europe 1 0 0 0 from 1 1 0 0 his 1 1 1 0 now 0 0 0 1 obama 1 0 0 0 president 1 1 1 0 returned 1 1 1 0 something 0 0 0 1 the 0 0 1 0 to 1 1 0 0 trip 1 1 1 0 How would we then caclulate distances? The simplest distance to calculate will be Manhattan (or city-block) distance. Essentially, for each word/term, we need to substract the frequency in text 1 from the frequency in text 2. 1 2 Manhattan Distance 1-2 about 0 0 0 and 0 0 0 bush 0 1 |-1| china 0 1 |-1| completely 0 0 0 different 0 0 0 europe 1 0 1 from 1 1 0 his 1 1 0 now 0 0 0 obama 1 0 1 president 1 1 0 returned 1 1 0 something 0 0 0 the 0 0 0 to 1 1 0 trip 1 1 0 Then we simply add up absolute values (i.e., treating negative values in the last column as positive) — and that is the distance value: Manhattan distance between sentence 1 and sentence 2 is 4. The symmetric matrix below shows Manhattan distances for all four sentences. These values show that sentences 1, 2, and 3 are quite similar, while sentence 4 is very different from 1, 2, and 3. Table 10.2: Symmetric Matrix with Distances 1 2 3 4 1 0 4 5 14 2 4 0 5 14 3 5 5 0 11 4 14 14 11 0 If we apply filtering to the first three sentences, removing words with df=1, you can see that the first two sentences become identical (Manhattan distance = 0), and the sentence 3 is very close: all three talking about the president returning from his trip: President Obama returned from his trip to Europe. President Bush returned from his trip to China. The president returned from his trip. Other distance measures. There is a variety of different distances that are meant to be more efficient in particular circumstances. Most common distances are Euclidean, Manhattan, and cosine. By and large, cosine distance/similarity8 would be your most optimal option. Main reasons are: 1) both Euclidean and Manhattan tend to give more weight to the most frequent terms; 2) neither Euclidean nor Manhattan are normalized (in their original forms), i.e their values are between 0 and infinity, while cosine distance values are between 0 and 1; 3) since cosine distance focuses on an angle between the vectors of texts in multidimensional space, rather than on the actual distance, it yields better results for texts of varying lengths on same topics (for example, between monographs and articles). You can read more about differences between these distance measures in John Ladd’s “Understanding and Using Common Similarity Measures for Text Analysis” at Programming Historian (Ladd 2020). The image below offers a graphical representation of these three distances, where A and B are two different texts and y and x are words — y occurs 5 times in A and 2 times in B; x occurs 2 times in A and 4 times in B. Why Manhattan (city-block) distance is called that way? 10.3 Practicalities Only a mere few years ago we would have had to write our own code for each and every step described above. Luckily, we now python libraries that efficiently do all the heavy lifting for us. One such library is sklearn; another library which is very useful is pandas (make sure to install both!). Below you can find all relevant code snippets which you will need to put together to complete this part. What do we need to generate? You will need to put together the following code snippets in order to produce two json files. The first one must contain top keywords for each publication. The second one must contain distances between publications. If you do everything as expected your results will look like the following: File with tf-idf keywords: { &quot;AdamsShepherds2006&quot;: { &quot;adams&quot;: 0.1272164898033877, &quot;administrative&quot;: 0.099074881442255, &quot;animals&quot;: 0.07099486255345346, &quot;bala&quot;: 0.08924612054231096, &quot;barley&quot;: 0.06046074109305735, &quot;corvée&quot;: 0.09158504454505918, &quot;cuneiform&quot;: 0.09386636316636131, &quot;dagan&quot;: 0.061182271700903075, &quot;dec&quot;: 0.09320481910701102, &quot;dynasty&quot;: 0.09595469820881344, &quot;fattening&quot;: 0.052203662027505905, &quot;flocks&quot;: 0.05101452549985564, &quot;gangs&quot;: 0.057181428331652115, &quot;herds&quot;: 0.09616271243655308, &quot;husbandry&quot;: 0.0958209157684203, &quot;iii&quot;: 0.13276591518791975, &quot;labor&quot;: 0.12346932617991906, &quot;lagash&quot;: 0.14169565407465887, &quot;mcc&quot;: 0.13380706013477167, &quot;pasturage&quot;: 0.0587244796687148, &quot;population&quot;: 0.05990187305273483, &quot;presently&quot;: 0.05103796476580478, &quot;prosopographic&quot;: 0.052203662027505905, &quot;province&quot;: 0.05866419351034984, &quot;records&quot;: 0.10031873550890447, &quot;robert&quot;: 0.07360610776912387, &quot;scribal&quot;: 0.07822478752418302, &quot;sheep&quot;: 0.15851065662360858, &quot;shepherd&quot;: 0.06216523460065599, &quot;shepherds&quot;: 0.33326918149698254, &quot;steinkeller&quot;: 0.17951743245184246, &quot;tue&quot;: 0.08198952007025857, &quot;umma&quot;: 0.43749736307561765, &quot;ur&quot;: 0.37598501212878616, &quot;wool&quot;: 0.11190732190242117 }, &quot;GarfinkleShepherds2004&quot;: { &quot;babylonian&quot;: 0.07650357591327776, &quot;barley&quot;: 0.09592656463776685, &quot;consumptive&quot;: 0.05337901001432162, &quot;credit&quot;: 0.08335590154194376, &quot;creditor&quot;: 0.13067922107308225, &quot;creditors&quot;: 0.07268117801701256, &quot;customary&quot;: 0.10111479309463182, &quot;debtor&quot;: 0.11012714465515581, &quot;duration&quot;: 0.056149153215818835, &quot;garfinkle&quot;: 0.12961758742186816, &quot;gin&quot;: 0.08170723635705109, &quot;iii&quot;: 0.1513526344445101, &quot;institutional&quot;: 0.07383767826894902, &quot;labor&quot;: 0.08619402867315176, &quot;lending&quot;: 0.10344979020528536, &quot;loan&quot;: 0.356626414161827, &quot;loans&quot;: 0.5995451434314758, &quot;mesopotamia&quot;: 0.05251561096144549, &quot;nippur&quot;: 0.06739596933652772, &quot;phrase&quot;: 0.05161099475107544, &quot;ra&quot;: 0.09626067026962869, &quot;rate&quot;: 0.06123888031779106, &quot;repaid&quot;: 0.06660264626574453, &quot;repayment&quot;: 0.1252001522028668, &quot;shepherds&quot;: 0.06507845149950617, &quot;si&quot;: 0.06021635652206201, &quot;silver&quot;: 0.08912141278136604, &quot;steinkeller&quot;: 0.0744916541514434, &quot;steven&quot;: 0.05335019010356139, &quot;sé&quot;: 0.07000447780472778, &quot;ur&quot;: 0.42847893151061767, &quot;witnesses&quot;: 0.06663117140384797 }, } File with distances (more correctly: cosine similarities): { &quot;AbdullahMandaean2018&quot;: { &quot;AlonSheikh2016&quot;: 0.2915990327301636, &quot;MinawiRhetoric2015&quot;: 0.277281021038338, &quot;PetriatCaravan2019&quot;: 0.27332888217367635, &quot;RiedlerCommunal2018&quot;: 0.25970501516946726, &quot;SaracogluReview2013&quot;: 0.2882066212119597 }, &quot;AdamsShepherds2006&quot;: { &quot;GarfinkleReview2005&quot;: 0.3542751363967858, &quot;GarfinkleShepherds2004&quot;: 0.30286604455405086, &quot;ReidRunaways2015&quot;: 0.2812514791059143, &quot;WidellReflections2005&quot;: 0.27154646436723745, &quot;ZettlerReconstructing2003&quot;: 0.32977887642906156 } } Note: in both files higher values mean more relevance and importance; in the second example (AdamsShepherds2006) all algorithmically identified articles are dealing with ancient Mesopotamian history, and even more specifically, with the Third Dynasty of Ur (See, Adams 2006; Garfinkle 2005, 2004; Reid 2015; Widell 2005; Zettler 2003). Keep in mind that tf-idf keywords are heavily filtered in the example. 10.4 Code Snippets The following code snippets are fully working and functional. All you need to do is to put them together properly. 10.4.1 Aggregating publications into a corpus The code snippet below processes all .json files from our memex (remember that we saved our OCR results using this extension; it might be better to use some unique extension though, something like .OCRED, so that we clearly keep different types of files apart). The code below 1) generates a dictionary with citekeys as keys and paths to .json files as values; 2) it then loops through citekeys and, 3) with each iteration of the loop, it loads each OCRed results and updates two lists: one with citekeys and another with texts. The result is all our data sorted into two lists, where items with the same index position are the citation key and the text of the same publication. We will use these lists to generate a table, similar to Table 10.1 above. Look carefully at the code. What else does this code do? ocrFiles = functions.dicOfRelevantFiles(pathToMemex, &quot;.json&quot;) citeKeys = list(ocrFiles.keys()) docList = [] docIdList = [] corpusDic = {} for citeKey in citeKeys: docData = json.load(open(ocrFiles[citeKey])) docId = citeKey doc = &quot; &quot;.join(docData.values()) doc = re.sub(r&#39;(\\w)-\\n(\\w)&#39;, r&#39;\\1\\2&#39;, doc) doc = re.sub(&#39;\\W+&#39;, &#39; &#39;, doc) doc = re.sub(&#39;\\d+&#39;, &#39; &#39;, doc) doc = re.sub(&#39; +&#39;, &#39; &#39;, doc) docList.append(doc) docIdList.append(docId) 10.4.2 Getting tf-idf Values and Distance Matrix with sklearn Now that we have our texts prepared, we can use the sklearn library to do all the tricky transformations (for detailed documentation on this library see official user guide); we will also need library pandas. We need to make sure that both librares are installed and proper modules are loaded in the following manner: import pandas as pd from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer) from sklearn.metrics.pairwise import cosine_similarity The sklearn library condenses rather elaborate code into just a few lines. The following lines of code convert our data into a format similar to what you can see in Table 10.1 above. The only difference is that instead of frequencies we will have tf-idf values — these results are saved into the variable vectorized (as sparse matrix). The last line of code generates cosine distances — these results are saved into the variable cosineMatrix (a symmetric matrix, similar to what you can see in Table 10.2 above) vectorizer = CountVectorizer(ngram_range=(1,1), min_df=5, max_df=0.5) countVectorized = vectorizer.fit_transform(docList) tfidfTransformer = TfidfTransformer(smooth_idf=True, use_idf=True) vectorized = tfidfTransformer.fit_transform(countVectorized) # https://en.wikipedia.org/wiki/Sparse_matrix cosineMatrix = cosine_similarity(vectorized) Perhaps the most important line to understand in the snippet above is the very first one where we create our own vectorizer: here we can adjust parameters to get different results. (More on parameters here). ngram_range can be defined as follows (To remind you of what ngram is, see Figure 10.1): (1, 1) only unigrams (single tokens) will be considered; (1, 2) means that both unigrams and bigrams will be considered; (2, 2) means only bigrams. Figure 10.1: What is ngram? Source: StackOverflow min_df and max_df are parameters to filter out terms with document frequency below (max_df) or above (min_df) certain threshold. (Using float between 0 and 1 will filter out specific percentage.) min_df is defined as 5, meaning that all words that occur in less than 5 documents will be excluded. max_df is defined as 0.5, meaning that words that occur in more than half of all documents will be excluded as well. Note: It makes sense to experiment with these parameters (defining them in the settings.yml will make it easier to experiment). 10.4.3 Converting Results sklearn generates needed results with just a few lines of code, which runs very fast (even with thousands documents). In order to make calculations fast, sklearn (as well as other similar libraries) use matrices (both symmetric and sparse), which are not exactly human readable. For example, my memex includes 1,140 documents, so the size of the distance matrix will be 1,140 x 1,140. The size of the tf-idf matrix will be 1,140 x 381,742! If we apply filtering (as in the code above), the tf-idf matrix shrinks quite significantly — down to 1,140 x 56,073 — but this is still not exactly usable. Most generated results are not really interesting. Matrices and dataframes (or tables) are rectangular and, for this reason, are quit difficult to filter. So, we need to convert this data in a more manageable format, filter out values outside of a specific range, and then save into some easy-to-use format that we can incorporate into our memex. The following lines of code show how to do the conversion into a dictionary. Your task will be to write code that loops through the dictionary and filters out irrelevant results. With sklearn we have created two matrices: the first one — vectorized — is a sparse matrix of tf-idf values; and the second one — cosineMatrix — is a symmetric matrix of cosine distances. These two types of matrices have different format and have to be processed slightly differently. (You can think of a sparse matrix as a sort-of compressed matrix, which stores only non-zero values; again, check this description). Converting a sparse matrix into a dataframe, and then into a dictionary (the optional line prints out the shape of your dataframe, i.e. the number of columns and rows, which will give you a good idea about the size of your data). tfidfTable = pd.DataFrame(vectorized.toarray(), index=docIdList, columns=vectorizer.get_feature_names()) print(&quot;tfidfTable Shape: &quot;, tfidfTable.shape) # optional tfidfTable = tfidfTable.transpose() tfidfTableDic = tfidfTable.to_dict() Converting a symmetric matrix into a dataframe, and then into a dictionary (the optional line prints out the shape of your dataframe, i.e. the number of columns and rows, which will give you a good idea about the size of your data). cosineTable = pd.DataFrame(cosineMatrix) print(&quot;cosineTable Shape: &quot;, cosineTable.shape) # optional cosineTable.columns = docIdList cosineTable.index = docIdList cosineTableDic = cosineTable.to_dict() Now you have dictionaries of both datasets! You already know how to save dictionaries into JSON files, but you need to filter these dictionaries first. Task: write the code that filters these dictionaries. Both dictionaries have the same structure, so you should be able to use the same function on both of them. I would recommend that with tf-idf terms you keep only words with value at least 0.05. For distance measures — include items with value at least 0.25 (keep in mind that this data will also include matches of each publications against itself – with value ~1 – so you would want to filter out such matches). In both cases you may have to experiment with the parameters to get usable results. Important: do not use .json extension, because it will conflict with other parts of our code, as we assume that all files with extension .json store OCR results. 10.5 Homework the main task is to generate two json files with the following: the first one must include main keywords for each publications (identified with the tf-idf approach). the second one must include distances between all publications in your memex (you can/should filter out irrelevant matches — this will make the file with results smaller and more manageable). additionally, take the solution scripts from the previous lesson and annotate every line of code; submit your annotations together with the main assignment; if you have any suggestions for improvements, please share them (this will count as extra points :). upload your results to your memex github repository place annotated scripts into _misc subfolder 10.6 Homework Solution stay tuned References "],
["syllabus.html", "Syllabus Course Details Aims, Contents and Method of the Course Course Evaluation Class Participation Homework Assignments Final Project Study materials Software, Tools, &amp; Technologies Schedule Lesson Topics", " Syllabus Course: 070172-1 UE Methodological course - Introduction to DH: Tools &amp; Techniques (2020W) Memex Edition Instructor: Dr. Maxim Romanov, maxim.romanov@univie.ac.at Language of instruction: English Office hours: Tu 14:00-15:00 (on Zoom; please, contact beforehand!) Office: Department of History, Maria-Theresien-Straße 9, 1090 Wien, Room 1.10 Course Details u:find Link: https://ufind.univie.ac.at/en/course.html?lv=070172&amp;semester=2020W Meeting time: Tu 09:00-10:30 Meeting place: Seminarraum Geschichte 3 Hauptgebäude, 2.Stock, Stiege 9; due to COVID, all meetings will be held online Aims, Contents and Method of the Course Back in 1945, Vannevar Bush, a Director of the US Office of Scientific Research and Development, proposed a device, which he called memex: Consider a future device … in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory. … The owner of the memex, let us say, is interested in the origin and properties of the bow and arrow. Specifically he is studying why the short Turkish bow was apparently superior to the English long bow in the skirmishes of the Crusades. He has dozens of possibly pertinent books and articles in his memex. First he runs through an encyclopedia, finds an interesting but sketchy article, leaves it projected. Next, in a history, he finds another pertinent item, and ties the two together. Thus he goes, building a trail of many items. Occasionally he inserts a comment of his own, either linking it into the main trail or joining it by a side trail to a particular item. When it becomes evident that the elastic properties of available materials had a great deal to do with the bow, he branches off on a side trail which takes him through textbooks on elasticity and tables of physical constants. He inserts a page of longhand analysis of his own. Thus he builds a trail of his interest through the maze of materials available to him. And his trails do not fade. Several years later, his talk with a friend turns to the queer ways in which a people resist innovations, even of vital interest. He has an example, in the fact that the outraged Europeans still failed to adopt the Turkish bow. In fact he has a trail on it. A touch brings up the code book. Tapping a few keys projects the head of the trail. A lever runs through it at will, stopping at interesting items, going off on side excursions. It is an interesting trail, pertinent to the discussion. … — The Atlantic, July 1945; YouTube: https://www.youtube.com/watch?v=c539cK58ees. The memex machine is often thought of as a precursor of the Internet. Be it as it may, the idea of a personal knowledge device is still of great relevance and of great importance to scholars and scientists whose job is to construct such trails on a regular basis. Needless to say that historians will benefit greatly from having such a machine at their disposal. The course will introduce you to basic, intermediate, and some advanced computational techniques, which will allow you to build and maintain your own digital memex machine. No prior programming experience is expected (we will be learning Python). Each class session will consist in large part of practical hands-on exercises led by the instructor. Laptops are required for the course. We will accommodate whatever operating system you use (Windows, Mac, or Linux), but it must be a laptop rather than a tablet. Course Evaluation Course evaluation will be a combination of in-class participation (30%), weekly homework assignments (50%), and the final project (20%). Class Participation Attendance is required; regular participation is the key to completing the course; all students must come with their laptops; homework assignments must be submitted on time (some can be completed later as a part of the final project, but this must be discussed with the instructor whenever the issue arises); the final project must be submitted on time. Homework Assignments Homework assignments are to be submitted by the beginning of the next class; These must be emailed to the instructor as attachments; In the subject of your email, please, use the following format: CourseID-LessonID-HW-Lastname-matriculationNumber, for example, if I were to submit homework for the first lesson, my subject header would look like: 070112-L01-HW-Romanov-12435687. DH is a collaborative field, so you are most welcome to work on your homework assignments in groups, however: you must still submit it. That is, if a groups of three works on one assignment, there must be three separate submissions emailed from each member’s email. Final Project The final project is your own memex machine, which can help you with your studies and your research. Your final project must include all working scripts that will allow you in the future to continuously expand your memex machine by adding new readings into the mix. You are most welcome to work on this final project in groups, but everybody is required to produce their own working machine. Study materials MAIN TEXTBOOK Zelle, John M. Python Programming: An Introduction to Computer Science. Third edition. Portland, Oregon: Franklin, Beedle &amp; Associates Inc, 2017. (access via Moodle); (Zelle 2017) We will focus primarily on learning how to work with python, which is one of the most popular programming languages used in digital humanities. We will use several resources and the emphasis will be on you studying on your own: partially, this is because of time constraints, but more importantly, you will need to acquire a skill of learning on your own. No worries, I will provide necessary help whenever needed. This textbook will be our main resource. It is well written and will help you to wrap your heads around important computer science concepts; this reading is crucial and without it many interactive tutorials out there will not be particularly helpful. Each chapter has assignments and self-test multiple choice sections; Supplementary materials are available at the publisher’s website, where you can download example code and end-of-chapter solutions; additionaly, you can find videos with complimentary instructions Additional: Paul Vierthaler’s “Hacking the Humanities Tutorials” (Python+): https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17 https://www.codecademy.com/learn/learn-python :: you can use this free interactive Python course; it, however, uses Python 2.x, while the main textbook focuses on Python 3.x; the course is still a good supplementary practice. ADDITIONAL MATERIALS https://www.codecademy.com Codecademy has a series of free course that you are encouraged to use for specific skills and technologies: https://www.codecademy.com/learn/learn-how-to-code https://www.codecademy.com/learn/learn-python https://www.codecademy.com/learn/learn-html https://www.codecademy.com/learn/introduction-to-regular-expressions https://www.codecademy.com/learn/learn-css https://programminghistorian.org/lessons/ “Programming Historian” offers a number of tutorials for aspiring digital humanists. These will be assigned to you as reference materials. You also are encouraged to explore those tutorials that are not included into the course. Software, Tools, &amp; Technologies The following is the list of software, applications and packages that we will be using in the course. Make sure to have them installed by the class when we are supposed to use them. Zotero, https://www.zotero.org/ [Mac] Terminal / [Windows] Powershell (both are already on your machines) Python https://www.python.org/, install the latest 3.x version git and https://github.com/, version control system pandoc (https://pandoc.org/), markdown, bibTex (bibliographical format for LaTeX) Regular expressions; (Sublime Text, https://www.sublimetext.com/ is a text editor which supports regular expressions) Wget (https://www.gnu.org/software/wget/), a free software package for retrieving files Understanding formats: [TEI] XML, csv/tsv, json, yml, etc. Creating: HTML, css, tiny snippets of Javascript Schedule Location: Seminarraum Geschichte 3 Hauptgebäude, 2.Stock, Stiege 9; due to COVID, all meetings will be held online via video-conferencing Tuesday 06.10. 09:00 - 10:30 Tuesday 13.10. 09:00 - 10:30 Tuesday 20.10. 09:00 - 10:30 Tuesday 27.10. 09:00 - 10:30 Tuesday 03.11. 09:00 - 10:30 Tuesday 10.11. 09:00 - 10:30 Tuesday 17.11. 09:00 - 10:30 Tuesday 24.11. 09:00 - 10:30 Tuesday 01.12. 09:00 - 10:30 Tuesday 15.12. 09:00 - 10:30 Tuesday 12.01. 09:00 - 10:30 Tuesday 19.01. 09:00 - 10:30 Tuesday 26.01. 09:00 - 10:30 Lesson Topics === CORE TOOLS &amp; METHODS === [ #01 ] Introduction &amp; Roadmap; Managing Bibliography with Zotero [ #02 ] Getting to Know the Command Line; Getting Started with Python [ #03 ] Version Control and Collaboration [ #04 ] Sustainable [Academic] Writing [ #05 ] Constructing Robust Searches / Optional: Basics of Webscraping [ #06 ] Understanding Structured Data and Major Formats === BUILDING MEMEX === [ #07 ] Parsing and Manipulating Bibliographic Data [ #08 ] Processing PDFs: OCR [ #09 ] View and Display: Simple HTML-based Interface [ #10 ] Summarizing Textual Data: Keyword Extraction [ #11 ] Finding Connections: Similarity Measures [ #12 ] Processing Everything Together: Batch Processing and re-Processing [ #13 ] Improving the Overall Memex Design: What Else Can We Add? Note: one of the classes might be canceled; this will be announced separately. Lesson materials will be appearing on the website shortly before each class. Lessons will be accessible via the Lessons link on the left panel. References "],
["references.html", "References", " References "]
]
